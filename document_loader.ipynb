{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e50c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "\n",
    "arquivo = \"./files/comunicando.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(arquivo)\n",
    "documento = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8101952b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd208eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-07-10T11:05:46-07:00', 'author': 'Lucas Abner', 'moddate': '2025-07-10T11:05:46-07:00', 'source': './files/comunicando.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='CONSUMINDO LLMs DENTRO DO MARVIN \\n \\nVamos descrever nessa documentação como enviar arquivos/containers para outra máquina e a \\nfazer túnel entre portas. \\n \\n1.  Enviando Container para o Marvin \\nAnteriormente, criamos um container singularity, onde está configurado para uso de GPUs \\ndedicados ao Ollama. Então vamos passar esse arquivo para nosso HPC para que ele rode o Ollama \\ne requisite direto do Servidor. Para mandar arquivos de sua máquina para outra é bem simples: \\nscp ollama.sif <seu_usuario>@<servidor_hpc>:<caminho_para_deixar_arquivo> \\nO que estamos fazendo aqui? Estamos transferindo uma cópia da sua máquina para o hpc, onde: \\n• scp = é a cópia que você está fazendo \\n• Ollama.sif = nome do arquivo que você quer transferir (definir o caminho do arquivo) \\n• <seu_usuario> = seu usuario no HPC \\n• <servidor_hpc> = nome do HPC \\n• <caminho_para_deixar_arquivo> = onde você quer deixar o arquivo no HPC \\nDessa maneira você consegue enviar seu container para as máquinas. \\n \\n \\n2. Túnel Entre Portas Ollama no Marvin \\nNo passo anterior, nós enviamos o container já buildado com o ollama para o servidor HPC(Marvin). \\nAgora como próximo passo, vamos criar um túnel de conexão entre a porta ollama da sua máquina \\n(:11434) com a porta ollama do container que está no Marvin: \\nssh –L 11434:localhost:11434 <seu_usuario>@<servidor_hpc> \\nAqui, estamos: \\n• ssh = sintexe que faz conexão segura entre duas máquinas \\n• -L = você está pedindo para escutar a sua porta local e a porta do HPC \\n• Porta 11434 = Esta é a porta no seu computador local que o SSH irá \"escutar\". Qualquer \\nconexão feita a esta porta no seu computador local será encaminhada através do túnel SSH. \\nVocê deve escolher uma porta livre no seu sistema (geralmente acima de 1024 para evitar \\nportas de sistema).')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documento[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "832a7145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'PyPDF',\n",
       " 'creator': 'Microsoft Word',\n",
       " 'creationdate': '2025-07-10T11:05:46-07:00',\n",
       " 'author': 'Lucas Abner',\n",
       " 'moddate': '2025-07-10T11:05:46-07:00',\n",
       " 'source': './files/comunicando.pdf',\n",
       " 'total_pages': 3,\n",
       " 'page': 2,\n",
       " 'page_label': '3'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documento[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c35788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39470/1732760218.py:6: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  chain = load_qa_chain(llm=chat, chain_type=\"stuff\", verbose=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "chat = ChatOllama(model=\"mistral:latest\")\n",
    "\n",
    "chain = load_qa_chain(llm=chat, chain_type=\"stuff\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "446049bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "CONSUMINDO LLMs DENTRO DO MARVIN \n",
      " \n",
      "Vamos descrever nessa documentação como enviar arquivos/containers para outra máquina e a \n",
      "fazer túnel entre portas. \n",
      " \n",
      "1.  Enviando Container para o Marvin \n",
      "Anteriormente, criamos um container singularity, onde está configurado para uso de GPUs \n",
      "dedicados ao Ollama. Então vamos passar esse arquivo para nosso HPC para que ele rode o Ollama \n",
      "e requisite direto do Servidor. Para mandar arquivos de sua máquina para outra é bem simples: \n",
      "scp ollama.sif <seu_usuario>@<servidor_hpc>:<caminho_para_deixar_arquivo> \n",
      "O que estamos fazendo aqui? Estamos transferindo uma cópia da sua máquina para o hpc, onde: \n",
      "• scp = é a cópia que você está fazendo \n",
      "• Ollama.sif = nome do arquivo que você quer transferir (definir o caminho do arquivo) \n",
      "• <seu_usuario> = seu usuario no HPC \n",
      "• <servidor_hpc> = nome do HPC \n",
      "• <caminho_para_deixar_arquivo> = onde você quer deixar o arquivo no HPC \n",
      "Dessa maneira você consegue enviar seu container para as máquinas. \n",
      " \n",
      " \n",
      "2. Túnel Entre Portas Ollama no Marvin \n",
      "No passo anterior, nós enviamos o container já buildado com o ollama para o servidor HPC(Marvin). \n",
      "Agora como próximo passo, vamos criar um túnel de conexão entre a porta ollama da sua máquina \n",
      "(:11434) com a porta ollama do container que está no Marvin: \n",
      "ssh –L 11434:localhost:11434 <seu_usuario>@<servidor_hpc> \n",
      "Aqui, estamos: \n",
      "• ssh = sintexe que faz conexão segura entre duas máquinas \n",
      "• -L = você está pedindo para escutar a sua porta local e a porta do HPC \n",
      "• Porta 11434 = Esta é a porta no seu computador local que o SSH irá \"escutar\". Qualquer \n",
      "conexão feita a esta porta no seu computador local será encaminhada através do túnel SSH. \n",
      "Você deve escolher uma porta livre no seu sistema (geralmente acima de 1024 para evitar \n",
      "portas de sistema).\n",
      "\n",
      "• Localhost:11434 = Este é o endereço IP ou nome do host da máquina de destino que você \n",
      "quer acessar e a porta no :11434 para a qual as conexões serão encaminhadas. \n",
      "• <seu_usuario>@<servidor_hpc> = Este é o servidor para o qual você está se conectando via \n",
      "SSH. \n",
      "Caso queira que o terminal só fique escutando aquela porta da máquina remota, acrescente um –N \n",
      "antes do –L, então o terminal ficara estático escutando aquela porta. \n",
      " \n",
      " \n",
      "3. Ligando Ollama e Requisitando pelo Túnel \n",
      "Agora que temos o container em nossa máquina remota, e já fizemos o túnel necessario para escutar \n",
      "a requisição ollama, é preciso ativar esse container para conseguir usar a sua LLM no projeto, certo? \n",
      "Então vamos lá: \n",
      "singularity run –-nv ollama.sif & \n",
      "Viu? De maneira simples já colocamos nosso container para funcionar. Esse // & // comercial é para \n",
      "que você possa usar o mesmo terminal para fazer outras requisições. Mas ainda falta baixar o \n",
      "modelo LLM para requisitar, então vamos fazer: \n",
      "singularity exec ollama.sif ollama run <modelo_llm> \n",
      "Agora sim, temos o modelo pronto na máquina remota para realizar requisições. \n",
      " \n",
      " \n",
      "4. Como Requisitar na sua Máquina? \n",
      "Da sua máquina, você vai realizar da mesma maneira que foi citado anteriormente: \n",
      "singularity exec ollama.sif ollama run <modelo_llm> \n",
      "Dessa maneira instalamos o modelo no container através de sua máquina, para aí sim você pode \n",
      "usar dentro de um projeto para requisitar. Com tudo configurado, agora é possível enviar requisições \n",
      "diretamente da sua máquina como se o Ollama estivesse instalado localmente. \n",
      "Exemplo com `curl` para gerar uma resposta: \n",
      "curl http://localhost:11434/api/generate -d '{ \n",
      "\"model\": \"llama3\", \n",
      "\"prompt\": \"Explique o que é aprendizado de máquina.\", \n",
      "\"stream\": false\n",
      "Human: Tem algo falando sobre Ollama?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' This document describes how to send files or containers to another machine and create a tunnel between ports. Here\\'s the summary:\\n\\n1. Sending Container to Marvin: You can transfer an Singularity container file from your local machine to the HPC (Marvin) using the command `scp ollama.sif <your_username>@<server_hpc>:<path_to_save_file>`. This command copies the specified file from your machine to the HPC.\\n\\n2. Tunnel Between Ports Ollama on Marvin: After sending the container, you\\'ll create a connection tunnel between the port ollama on your local machine (port 11434) and the port ollama of the container running on Marvin using the command `ssh –L 11434:localhost:11434 <your_username>@<server_hpc>`. This command sets up a secure connection between your local machine and the HPC.\\n\\n3. Linking Ollama and Requisition via Tunnel: With the container on the remote machine and the necessary tunnel created, you need to activate the container to use your LLM in the project. Use the commands `singularity run –-nv ollama.sif &` and then `singularity exec ollama.sif ollama run <model_llm>` to make the container work and load the LLM model for requests.\\n\\n4. Requesting on Your Machine: To request from your machine, use the same command as mentioned earlier: `singularity exec ollama.sif ollama run <model_llm>`. This way, you can install the model into the container through your local machine, allowing you to use it within a project for requests. An example using curl to generate a response is provided in the document:\\n`curl http://localhost:11434/api/generate -d \\'{  \"model\": \"llama3\",  \"prompt\": \"Explique o que é aprendizado de máquina.\",  \"stream\": false }`\\n\\nThis information is about using Ollama, a Singularity container for machine learning models.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pergunta = \"Tem algo falando sobre Ollama?\"\n",
    "chain.run(input_documents=documento[:2], question=pergunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd124260",
   "metadata": {},
   "source": [
    "## Carregando videos do Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7b5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
    "import whisper\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6351036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=00Q0G84kq3M\n",
      "[youtube] 00Q0G84kq3M: Downloading webpage\n",
      "[youtube] 00Q0G84kq3M: Downloading tv client config\n",
      "[youtube] 00Q0G84kq3M: Downloading tv player API JSON\n",
      "[youtube] 00Q0G84kq3M: Downloading ios player API JSON\n",
      "[youtube] 00Q0G84kq3M: Downloading m3u8 information\n",
      "[info] 00Q0G84kq3M: Downloading 1 format(s): 140\n",
      "[download] files/youtube/RAG vs. Fine Tuning.m4a has already been downloaded\n",
      "[download] 100% of    8.28MiB\n",
      "[ExtractAudio] Not converting audio files/youtube/RAG vs. Fine Tuning.m4a; file is already in target format m4a\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.youtube.com/watch?v=Tm9y9pM5dJc\"\n",
    "save_dir = \"files/youtube\"\n",
    "\n",
    "def transcribe_audio_local(audio_path):\n",
    "        model = whisper.load_model(\"base\")\n",
    "        result = model.transcribe(audio_path)\n",
    "        return Document(page_content=result[\"text\"])\n",
    "\n",
    "class LocalWhisperLoader(BaseLoader):\n",
    "    def __init__(self, urls, save_dir):\n",
    "        self.urls = urls\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def load(self):\n",
    "        loader = YoutubeAudioLoader(self.urls, self.save_dir)\n",
    "        docs = []\n",
    "        for blob in loader.yield_blobs():\n",
    "            audio_path = blob.path\n",
    "            doc = transcribe_audio_local(str(audio_path))\n",
    "            docs.append(doc)\n",
    "        return docs\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=00Q0G84kq3M\"\n",
    "loader = LocalWhisperLoader([url], \"files/youtube\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a2a244a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39470/1681284590.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(input_documents = docs, question = pergunta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      " In this course, Lance Martin will teach you how to implement RAG from scratch. Lance is a software engineer at Langchain. And Langchain is one of the most common ways to implement RAG. Lance will help you understand how to use RAG to combine custom data with LLMs. Hi, this is Lance Martin. I'm a software engineer at Langchain. And we're giving a short course, focused on RAG or a tree vlogmented generation, which is one of the most popular kind of ideas and applications in LLMs today. So really the motivation for this is that most of the world's data is private. Whereas LLMs are trained on publicly available data. So you can kind of see on the bottom on the x-axis, the number of tokens using pre-training various LLMs. So kind of various from say 1.5 trillion tokens in the case of smaller models like phi 2, out to some very large number that we actually don't know for proprietary models like GPT-4, cloud 3. But what's really interesting is that the context window or the ability to feed external information into these LLMs is actually getting larger. So about a year ago, context windows are between 4 and 8000 tokens. You know, that's like maybe a dozen pages of text. We've recently seen models all the way out to a million tokens, which is thousands of pages of text. So while these LLMs are trained on large-scale public data, it's increasingly feasible to feed them this huge mass of private data that they've never seen. That private data can be your kind of personal data, it can be corporate data, or you know other information that you want to pass to an LLM that's not natively in its training set. And so this is kind of the main motivation for RAD. It's really the idea that LLMs 1 are kind of the center of a new kind of operating system. And 2, it's increasingly critical to be able to feed information from external sources such as private data into LLMs for processing. So that's kind of overarching motivation for RAD. And now RAD refers to retrieval logmented generation. And you can think of it in three very general steps. There's a process of indexing of external data. So you can think about this as you know building a database, for example. Many companies already have large-scale databases in different forms. They could be SQL DBs, relational DBs. They could be vector stores or otherwise. But the point is that documents are indexed such that they can be retrieved based upon some heuristics relative to an input, like a question. And those relevant documents can be passed to an LLM and the LLM can produce answers that are grounded in that retrieved information. So that's kind of the centerpiece or central idea behind RAD. And why it's really powerful technology because really uniting the the knowledge and processing capacity of LLMs with large-scale private external data stores for which most of the important data in the world still lives. And in the following short videos, we're going to kind of build up a complete understanding of the RAD landscape. And we're going to be covering a bunch of interesting papers and techniques that explain kind of how to do RAD. And I've really broken it down into a few different sections. So starting with a question on the left, the first kind of section is what I call query tens of the translation. So this captures a bunch of different methods to take a question from a user and modify it in some way to make it better suited for retrieval from one of these indexes we've talked about. That can use methods like query writing. It can be decomposing the query into constituent subquestions. Then there's a question of routing. So taking that decomposed or revert in question and routing it to the right place, you might have multiple vector stores or relational DB, graph DB and a vector store. So that's the challenge of getting a question to the right source. Then there's kind of a challenge of query construction, which is basically taking natural language and converting it into the DSL necessary for whatever data source you want to work with. A classic example here is text-to-sequel, which is kind of a very kind of well-studied process, but text decipher for graph DB is very interesting. Text to metadata filters for vector DBs is also a very big area of study. Then there's indexing. So that's the process of taking your documents and processing them in some ways that can be easily retrieved. And there's a bunch of techniques for that. We'll talk through. We'll talk through different embedding methods. We'll talk about different indexing strategies. After retrieval, there are different techniques to re-rank or filter retrieved documents. And then finally, we'll talk about generation and kind of an interesting new set of methods to do what we might call as active rack. So in that retrieval or generation stage, grade documents, grade answers, grade for relevance to the question, grade for faithfulness to the documents, i.e. check for hallucinations, and if either fail, feedback, re-retrieve, or rewrite the question, regenerate the answer, and so forth. So there's a really interesting set of methods we're going to talk through that cover that like retrieval and generation with feedback. And in terms of general outline, we'll cover the basics first. It'll go through indexing, retrieval and generation, kind of in the bare bones. And then it'll talk through more advanced techniques that we just saw on the prior slide, queer transformations, routing, construction, and so forth. This is last from Lineshane. The second video in our series, Radclim Scratch, focused on indexing. So in the past video, you saw the main kind of overall components of rag pipelines, indexing, retrieval and generation. And here we're going to kind of deep dive on indexing, and let's just go quick overview of it. So the first aspect of indexing is we have some external documents that we actually want to load and put into what we're trying to call a retriever. And the goal of this retriever is simply given an input question, I want to fish out documents that are related to my question in some way. Now the way to establish that relationship or relevance or similarity is typically done using some kind of numerical representation of documents. And the reason is that it's very easy to compare vectors, for example, with numbers relative to, you know, just reformed text. And so a lot of approaches have been developed over the years to take text documents and compress them down into a numerical representation that then can be very easily searched. Now there's a few ways to do that. So Google and others came up with many interesting statistical methods where you take a document, you look at the frequency of words, and you build what they call sparse vectors, such that the vector locations are, you know, a large vocabulary of possible words, each value represents the number of occurrences of that particular word. And it's sparse because there's of course many zeros. It's a very large vocabulary relative to what's present in the document. And there's very good search methods over this type of numerical representation. Now a bit more recently embedding methods that are machine-learned. So you take a document and you build a compressed fixed length representation of that document, have been developed with correspondingly very strong search methods over embeddings. So the intuition here is that we take documents and we typically split them because embedding models actually have limited context windows. So, you know, on the order of maybe five, 12 tokens up to 8000 tokens are beyond, but they're not infinitely large. So documents are split and each document is compressed into a vector. And that vector captures a semantic meaning of the document itself. The vectors are indexed. Questions can be embedded in the exactly same way. And then a numerical kind of comparison in some form, you know, using very different types of methods can be performed on these vectors to fish out relevant documents relative to my question. And let's just do a quick code walkthrough on some of these points. So I have my notebook here. I've installed here. Now I've set a few API keys for Langsmith, which are very useful for tracing, which we'll see shortly. Previously, I walked through this kind of quick start that just showed overall how to lay out these rag pipelines. And here what I'll do is I'll deep dive a little bit more on indexing and I'm going to take a question and a document. And first, I'm just going to compute the number of tokens in, for example, the question. And this is interesting because embedding models and elements more generally operate on tokens. And so it's kind of nice to understand how large the documents are. I'm trying to feed in. And this gets obviously a very small in this case question. Now I'm going to specify open I embeddings. I specify an embedding model here. And I just say embed embed query. I can pass my question and my document. And what you can see here is that runs. And this is mapped to now a vector of length 1536. And that fixed length vector representation will be computed for both documents and really for any documents. So you're always kind of computing this fixed length vector that encodes the semantics of the text that you passed. Now I can do things like cosine similarity to compare them. And as we'll see here, I can load some documents. This is just like we saw previously. I can split them. And I can index them here just like we did before. But we can see under the hood, really what we're doing is we're taking each split where I'm betting it using open I embeddings into this kind of inspector representation. And then that's stored with a link to the raw document itself in our vector store. And next we'll see how to actually do retrieval using this vector store. This is Lance from Linechain. And this is the third video in our series rag from scratch building up a lot of the motivations for rag from the very basic components. So we're going to be talking about retrieval today. In the last two short videos, I outlined indexing and gave kind of an overview of this flow, which starts with indexing or documents retrieval documents relevant to our question and then generation of answers based on the retrieved documents. And so we saw that the indexing process basically makes documents easy to retrieve. And it goes through a flow that basically looks like you take our documents, you split them in some way into these smaller chunks that can be easily embedded. Those embeddings are numerical representations of those documents that are easily searchable. And they're stored in index when given a question that's also embedded, the index performs a similarity search and returns splits their relevant to the question. Now if we dig a little bit more under the hood, we can think about it like this. If we take a document and embed it, let's imagine that embedding just had three dimensions. So each document is projected into some point in this 3D space. Now the point is that the location in space is determined by the semantic meaning or content in that document. So to follow that then, documents in similar locations in space contain similar semantic information. And this very simple idea is really the cornerstone for a lot of searcher retrieval methods that you'll see with modern vector stores. So in particular, we take our documents, we embed them into this, in this case, a toy 3D space, we take our question, do the same. We can then do a search like a local neighborhood search, you can think about in this 3D space, around our question to say, hey, what documents are nearby? And these nearby neighbors are then retrieved because they have similar semantics relative to our question. And that's really what's going on here. So again, we took our documents, we split them, we embed them, and now they exist in this high dimensional space. We've taken our question and embedded it, projected in that same space, and we just do a search around the question for nearby documents and grab ones that are close. And we can pick some number, we can say, we want one or two or three or end documents close to my question in this embedding space. And there's a lot of really interesting methods that implement this very effectively. I link one here. And we have a lot of really nice integrations to play with this general idea. So many different embedding models, many of our indexes, lots of document loaders, and lots of splitters that can be kind of recombined to test different ways of doing this kind of indexing or retrieval. So now I'll show a bit of a code walkthrough. So here we defined, we kind of walked through this previously. This is our notebook. We've installed a few packages. We set a few environment variables using Langsmith. And we showed this previously. This is just an overview showing how to run Ragn like kind of end to end. In the last short talk, we went through indexing. And what I'm going to do very simply is I'm just going to reload our documents. So now I have our documents. I'm going to resplit them. And we saw before how we can build our index. Now here, let's actually do the same thing. But in the slides, we actually showed kind of notion of search in that 3D space. And a nice parameter to think about in building your retriever is K. So K tells you the number of nearby neighbors to fetch when you do that retrieval process. And we talked about, you know, in that 3D space, do I want one nearby neighbor or two or three? So here, we can specify K equals one. For example, now we're building our index. So we're taking every split embedding it, storing it. Now it's nice is I asked a question, what is task decomposition is related to the blog post? And I'm going to run get relevant documents. So I run that. And now how may do I get back? I get one as expected based on K equals one. So this retrieved document should be related to my question. Now I can go to Langsmith. And we can open it up and we can look at our retriever. And we can see here is our question. Here's a one document we got back. And okay, so that makes sense. This document pertains to task decomposition in particular. And it kind of lays out a number of different approaches that can be used to do that. This all kind of makes sense. And this shows kind of in practice how you can implement this KNN or K nearest neighbor search really easily just using a few lines of code. And next we're going to talk about generation. Thanks. Okay, this is Lance from Langshang. This is the fourth short video in our Rad from scratch series. This can be focused on generation. Now in the past few videos, we walk through the general flow for kind of basic rag, starting with indexing, followed by retrieval, then generation of an answer based upon the documents that we retrieved that are relevant to our question. This is kind of the very basic flow. Now an important consideration in generation is really what's happening is we're taking the documents you retrieve and we're stuffing them into the LM context window. So if we kind of walk back through the process, we take documents, we split them for convenience or embedding. We then embed each split and we store that in a vector store as this kind of easily searchable numerical representation or vector. And we take a question and embed it to produce a similar kind of numerical representation. We can then search, for example, using something like KNN in this kind of high dimensional space for documents that are similar to our question based on their proximity or location in this space. In this case, you can see 3D as a toy, kind of toy example. Now we've recovered relevant splits to our question. We pack those into the context window and we produce our answer. Now this introduces the notion of a prompt. So the prompt is kind of, you can then have a placeholder that has, for example, in our case, keys. So those keys can be like context and question. So there basically are buckets that we're going to take those retrieve documents and slot them in. We're going to take our question and also slot it in. And if you kind of walk through this flow, you can kind of see that we can build like a dictionary from our retrieve documents and from our question. And then we can basically populate our prompt template with the values from the dict. And then it becomes a prompt value, which can be passed to NLM, like a chat model, resulting in chat messages, which we then parse into a string and get our answer. So that's like the basic workflow that we're going to see. And let's just walk through that in code very quickly to kind of give you like a hands-on intuition. So we had our notebook. We walked through previously, install a few packages. I'm setting a few likes from environment variables. We'll see it's nice for kind of observing and debugging our traces. Previously, we did this quick start. We're going to skip that over. And what I will do is I'm going to build our retriever. So again, I'm going to take documents and load them. And then I'm going to split them here. We've kind of dumped this previously. So I'll go through this kind of quickly. And then we're going to embed them and store them in our index. So now we have this retriever object here. Now I'm going to jump down here. Now here's where it's kind of fun. This is the generation bit. And you can see here I'm defining something new. This is a prompt template. And my prompt template is something really simple. It's just going to say, answer the following question based on this context. It's going to have this context variable and a question. So now I'm building my prompt. So great. Now I have this prompt. Let's define an LLM. I'll choose 3.5. Now this is just the notion of a chain. So in Langshane, we have an expression language called L-C-E-L, Langshane expression language, which lets you really easily compose things like prompts, LMs, parsers, retrievers, and other things. But the very simple kind of example here is just, let's just take our prompt, which you defined right here, and connect it to an LLM, which you defined right here, into this chain. So there's our chain. Now all we're doing is we're invoking that chain. So every Langshane expression language chain has a few common methods like invoke, BAT stream. In this case, we just invoke it with a Dict. So context and question that maps to the expected keys here in our template. And so if we run invoke, what we see is it's just going to execute that chain and we get our answer. Now if we zoom over to Langshane, we should see that it's been populated. So yeah, we see a very simple, runable sequence. Here was our document, and here's our output, and here is our prompt. Answer the following question based in the context. Here's the document we passed in. Here is the question, and then we get our answer. So that's pretty nice. Now there's a lot of other options for rag prompts. I'll pull one in from our prompt hub. This one's like kind of a popular prompt. So it just like has a little bit more detail. But it's the main intuition is the same. You're passing in documents, you're asking a lot of the reason about the documents, given a question, producing an answer. And now here I'm going to find a rag chain, which will automatically do their retrieval for us. And all I have to do is specify here's my retriever, which we defined before. Here is our question we, which we invoke with. The question gets passed through to the key question in our dict, and it automatically will trigger the retriever, which will return documents, which get passed into our context. So it's exactly what we did up here, except before we did this manually. And now this is all kind of automated for us. We passed that DIC, which is auto populated into our prompt, LM, Alka parser, Nalton Vokit. And that should all just run. And great, we get an answer, and we can look at the trace. And we can see everything that happened. So we can see our retriever was run. These documents were retrieved. They get passed into our LM. And we get our final answer. So this kind of the end of our overview, where we talked about, I'll go back to the slides here quickly, we talked about indexing, retrieval, and now generation. And follow up short videos, we'll kind of dig into some of the more complex, your detailed themes that address some limitations that can arise in this very simple pipeline. Thanks. All right, slides from LMChain. Over the next few videos, we're going to be talking about query translation. And in this first video, we're going to cover the topic of multi query. So query translation sits kind of at the first stage of an advanced rag pipeline. And the goal of query translation is really just to take an input user question and to translate in some way in order to improve retrieval. So the problem statement is pretty intuitive. User queries can be ambiguous. And if the query is poorly written, because we're typically doing some kind of semantic similarity search between the query and our documents, if the query is poorly written or imposed, we won't retrieve the proper documents from our index. So there's a few approaches to attack this problem. And you can kind of group them in a few different ways. So here's one way I like to think about it. A few approaches has evolved query rewriting. So taking a query and reframing it, like writing it from a different perspective. And that's what we're going to talk about a little bit here in depth, using approaches like multi query or rag fusion, we'll talk about in the next video. You can also do things like take a question and break it down to make it less abstract, like into sub questions. And there's a bunch of interesting papers focused on that, like at least to most from Google. You can also take the opposite approach of take a question and make it more abstract. And there's actually approach we're going to talk about later in a future video called step by prompting with focuses on like kind of higher, higher level question from the input. So the intuition though for this multi query approach, so we're taking a question and we're going to break it down into a few differently worded questions from different perspectives. And the intuition here is simply that it is possible that the way a question is initially worded, once embedded, it is not well aligned or in close proximity in this high dimensional embedding space to a document we want to retrieve as actually related. So the thinking is that by kind of rewriting it in a few different ways, you actually increase the likelihood of actually retrieving the document that you really want to. Because of nuances in the way that documents and questions are embedded, this kind of more shotgun approach of taking a question, fanning it out into a few different perspectives, may improve and increase the reliability of the retrieval. That's like the intuition really. And of course we can combine this with retrieval so we can take our kind of fan down questions, do retrieve on each one and combine them in some way and perform rag. So that's kind of the overview. And now let's go over to our code. So this is a notebook and we're going to share all this. We're just installing a few packages. We're setting a lengthmith API keys, which we'll see why that's quite useful here shortly. There's our diagram. Now first, I'm going to index this blog post on agents. I'm going to split it. Well, I'm going to load it. I'm going to split it and then I'm going to index it in Chrome, locally. So this is a vector story done mispreviously. So now I have my index defined. So here's where I'm defining my prompt for multi-query, which is urinate, your assistant, your task is to basically reframe this question into a few different subquestions. So there's our prompt right here. We're past that to an LLM, parse it into a string and then split the string by new lines. And so we'll get a list of questions out of this chain. That's really all we're doing here. Now all we're doing is here's a sample input question. There's our generate queries chain, which we defined. We're going to take that list and then simply apply each question to a retriever. So we'll do retrieval per question. And this little function here is just going to take the unique union documents across all those retrievals. So what's run this and see what happens? So we're going to run this and we're going to get some set of questions or documents back. So let's go to length with now. We can actually see what happened under the hood. So here's the key point. We ran our initial chain to generate this set of refrained questions from our input. And here is that prompt. And here is that set of questions that we generated. Now what happened is for every one of those questions we did an independent retrieval. That's what we're showing here. So that's kind of the first step, which is great. Now I can go back to the notebook and we can show this working end to end. So now we're going to take that retrieval chain. We'll pass it into context of our final rag prompt. We'll also pass through the question. We'll pass that to our rag prompt here, pass it to an LM and then parc output. Now let's let's kind of see how that works. So again, that's okay. There it is. So let's actually go into length and see what happened under the hood. So this is our final chain. This is great. We took our input question. We broke it down to these like five refraised questions. For every one of those, we did a retrieval. That's all great. We then took the unique union of documents and you can see in our final LLM prompt, answer the following question based on the context. This is the final set of unique documents that we retrieved from all of our subquestions. Here's our initial question. There's our answer. So that kind of shows you I can set this up really easily. I can use length Smith to kind of investigate what's going on. And in particular, use length Smith to investigate those intermediate questions that you generate in that kind of question generation phase. And in the future talks, we're going to go through some of these other methods a week, kind of introduce the start of this one. Thank you. And last one, LM chain. This is the second video of our deep dive on query translation. In our rack from scratch series, focused on a method called rack fusion. So as we kind of showed before, query translation you can think of as the first stage in an advanced rag pipeline. We're taking an input user question and we're translating it in some way in order to improve retrieval. Now, we showed this general mapping of approaches previously. So again, you have kind of like rewriting. So you can take a question like kind of break it down into differently worded or different different perspectives of the same question. So that's kind of rewriting. There's sub questions we take a question break it down into smaller problems, stall between independently. And then there's step back, we take a question and kind of go more abstract where you kind of ask a higher level question as a precondition to answer the user question. So those are the approaches. And we're going to dig into one of the particular approaches for writing called rack fusion. Now, this is really similar to what we just saw with multi query. The difference being we actually apply kind of a clever ranking, ranking step, other retrieve documents, which you call reciprocal rank fusion. That's really the only difference. The input stage of taking a question breaking it out into a few kind of differently worded questions retrieval on each one is all the same. And we're going to see that in the code here shortly. So let's just hop over there and then look at this. So again, here is a notebook that we introduced previously. Here's the packages we've installed. We've set a few API keys for a length of what you see why is quite useful. And you can kind of go down here to a rack fusion section. And the first thing you'll know is what our prompt is. So it looks really similar to the prompt we just saw with multi query. And simply your helpful assistant that generates multiple search queries based upon the user input. And here's the question output for queries. So let's define our prompt. And here is our query generation. Shand again, this looks a lot like we just saw. We take our prompt, plummed into an LM, and then basically parsed by new lines. And that will basically split out these questions into a list. That's all it's going to happen here. So that's cool. Now here's where the novelty comes in. Each time we do retrieval from one of those questions or any get back a list of documents from our retrieval. And so we do it over that. We generate four questions here based on our prompt. We do the four questions, well, like a list of lists basically. Now reciprocal rank fusion is really well suited for this exact problem. We want to take this list and build a single consolidated list. And really all it's going on is it's looking at the documents in each list and kind of aggregating them into a final output ranking. And that's really the intuition around what's happening here. So let's go ahead and. So let's. So let's go ahead and look at that in some detail. So we can see we run. Retrieval. That's great. Now let's go over to Lang Smith and have a look at what's going on here. So we can see that here is our prompt, your helpful assistant that generates multiple search queries based on single input. And here is our search queries. And then here are our four retrievals. So that's that's really good. So we know that all is working. And then those retrievals simply went into this rank function and are forcefully ranked to a final list of six unique rank documents. That's really all we did. So let's actually put that all together into a full rag chain that's going to run retrieval. Return that final list of rank documents. And pass it to our context. Passed to our question. Send that to our rag prompt. Pass it to an LM. Pass it to an output. And let's run all that together and see that working. Cool. So there's our final answer. Now let's have a look in Lang Smith. We can see here was our four questions. Here's our retrievals. And then our final rag prompt plumb through the final list of rank six questions, which we can see laid out here. And our final answer. So this can be really convenient, particularly if we're operating across like maybe different vector stores. Or we want to do like retrieval across a large number of kind of differently worded questions. This reciprocal rank fusion step is really nice. For example, if we wanted to only take the top three documents or something, it can be really nice to build that consolidated ranking across all these independent retrievals that pass that to LM for the final generation. So that's really the intuition that what's happening here. Thanks. Hi, this is Lance Mlangchen. This is our third video focused on query translation in the rag from scrap series. And we're going to be talking about decomposition. So query translation in general is a set of approaches that sits kind of towards the front of this overall rag pipeline. And the objective is to modify a rewrite or otherwise decompose and input question from a user in order to improve retrieval. So we can't talk through some of these approaches previously in particular various ways to do query writing like rag fusion and multi query. There's a separate set of techniques that become pretty popular and are really interesting for certain problems, which you might call like kind of breaking down or decomposing an input question into set of sub questions. So some of the papers here that are pretty cool are, for example, this work from Google. And the objective really is first to take an input question and decompose it into set of sub problems. So this particular example from the paper was the problem of last letter concatenation. And so it took the input question of three words, think machine learning and broke it down into three sub problems, think, think machine, think machine learning as the third sub problem. And then you can see in this bottom panel, it solves each one individually. So it shows, for example, in green, solving the problem with fake machine, where you can catenate the last letter of k with the last letter of machine or the last letter of think k, lesser machine, e, concatenate those two ke. And then for the overall problem, taking that solution and then a and basically building on it to get the overall solution of keg. So that's kind of one concept of decomposing into sub problems, solving them sequentially. Now a related work called IRCOT or interlead retrieval combines retrieval with chain of thought reasoning. And so you can kind of put these together into one approach, which you can think of as kind of dynamically retrieval to solve a set of sub problems, kind of that retrieval, kind of interleaving with chain of thought as noted in the second paper. And a set of decompose questions based on your initial question from the first work from Google. So really the idea here is we're taking one sub question, we're answering it, we're taking that answer and using it to help answer the second sub question and so forth. So let's actually just walk through this in code to show how those might work. So this is the notebook we've been working with from some of the other videos. You can see we already have our tree ver defined up here at the top. And what we're going to do is we're first going to find a prompt that's basically going to say given an input question, let's break it down to a sub problem, there's a question which can be solved individually. So we can do that and this blog post is focused on agents. So let's ask a question about what are the main components of an L-Empower and a Ponomous Agent System. So let's run this and see what the decompose questions are. So you can see the decompose questions are what is L-Empower and how does it work, what are those components, and then how the commos interact. So it's kind of a sane way to kind of break down this problem into a few sub problems which you might attack individually. Now here's where we define a prompt that very simply is going to take our question, we'll take any prior questions we've answered and we'll take our retrieval and basically just combine them. And we can define this very simple chain. Actually, let's go back and make sure our tree ver is defined up at the top. So now we are building our retriever. Good, we have that now. So we can go back down here and let's run this. So now we are running and what's happening is we're trying to solve each of these questions individually using retrieval and using any prior question answers. So okay, very good, looks like that's been done. And we can see here's our answer. Now let's go over to Langtooth and actually see what happened under the hood. So here's what's kind of interesting and helpful to see. For the first question, so here's our first one, it looks like it just does retrieval, which is what we expect and then use that to answer this initial question. Now for the second question should be a little bit more interesting because if you look at our prompt, here's our question. Now here is our background available question answer pair. So this was the answer question answer pair from the first question, which we add to our prompt. And then here's the retrieval for this particular question. So we're kind of building up the solution because we're pending the question answer pair from question one. And then likewise with question three, it should combine all of that. So we can look at here. Here's our question. Here's question one. Here's question two. Great. Now here's additional retrieval related to this particular question and we get our final answer. So that's like a really nice way you can kind of build up solutions using this kind of interleaved retrieval and concatenating prior question answer pairs. I do want to mention very briefly that we can also take a different approach where we can just answer these all individually and then just concatenate all those answers to produce a final answer. And I'll show that really quickly here. It's like a little bit less interesting maybe because you're not using answers from each question to inform the next one. You're just answering them all in parallel. This might be better for cases where it's not really like a sub-question decomposition, but maybe it's like a set of several independent questions whose answers don't depend on each other. That might be relevant for some problems. And we can go ahead and run. This ran as well. We can look at our trace. In this case, we can see that this actually just kind of concatenates all of our QA pairs to produce the final answer. So this gives you a sense for how you can use the query decomposition and point ideas from two different papers that are pretty cool. Thanks. This is the launch from Langcheng. This is the fourth video in our deep dive on Queer Translation in the Rag from Scratch series. And we're going to focus on step back prompting. So Queer Translation, as we said in some of the prior videos, kind of sits at the kind of first stage of kind of a Rag pipeline or flow. And the main aim is to take an input question and to translate it or modify in such a way that it improves retrieval. Now we talked through a few different ways to approach this problem. So one general approach involves rewriting a question and we talked about two ways to do that, Rag Fusion, Multi-Query. And again, this is really about taking a question and modifying it to capture a few different perspectives which may improve the retrieval process. Now another approach is to take a question and kind of make it less abstract, like break it down into sub questions and then solve each of those independently. So that's what we saw with like least to most prompting. And a bunch of other variants kind of in that vein of sub problem solving and they can solve those solutions into a final answer. Now a different approach presented by again Google as well is step back prompting. So step back prompting can take the opposite approach where it tries to ask a more abstract question. So the paper talks a lot about using few shot prompting to produce what they call these step back or more abstract questions. And the way it does it is it provides a number of examples of step back questions given your original question. So like this is for example there like for prompt template, your next pertinural knowledge, I ask you a question your response should be comprehensive not contradict with the following. And this is kind of where you provide your like original and then step back. So here's like some example questions. So like at your saw the creation of the region where the country is located, which region of the country is the county of of hercature related. Jensen knows born in what country what is Jensen knows personal history. So that's going to be more into an example. So it's like you ask a very specific question about like the country someone's born. The more abstract question is like just give me the general history of this individual without worrying about that particular, most visit question. So let's actually just walk through how this can be done in practice. So again, here's kind of like a diagram of the various approaches from less abstraction to more abstraction. Now here is where we're formulating our prompt using a few of the few shot examples from the paper. So again, like input, yeah, something about like the please for form of arrest and what what can members of the police do. So like it basically gives the model a few examples. We basically formulate this into a prompt, especially all it's going on here. Again, we repeat this overall prompt, which we saw from the paper, your next world knowledge, your test is to step back and paraphrase question. Generic step back question, which is easier to answer. Here's some examples. So it's like a very intuitive prompt. So okay, let's start with the question, what is task composition for L on agents? And we're going to say generate step back question. Okay, so this is pretty intuitive, right? What is a process of task composition? So like not worrying as much about agents, but what is a process of task composition in general? And then hopefully that can be independently retrieved. We can independently retrieve documents related to the step back question. And in addition, retreat documents related to the the actual question and combine those to produce kind of final answer. So that's really all it's going on. And here's the response template where we're plumbing in the step back context and our question context. And so what we're going to do here is we're going to take our input question and perform a treeble on that. We're also going to generate our step back question and perform a treeble on that. We're going to plum those into the prompt as here's our very here's our basically our prompt keys, normal question, step back question. And our overall question again, we form like those as a dict. We plum those into our response prompt. And then we go ahead and attempt to answer our overall question. So we're going to run that. That's running. And okay, we have our answer. Now I want to hop over to Langsmith and attempt to show you kind of what that looked like under the hood. So let's see, let's go into each of these steps. So here was our prompt, right? You're an expert in world knowledge, you're tasked to step back and paraphrase a question. So here were our few shot prompts. And this was our step back question. So what is a process of task composition? Good. From the input, what is the application for LN agents? We perform a retrieval on both. What is process task composition? And what is that? So I'm sure LN agents, we perform both retrievals. We then populate our prompt with both original question answer. And then here's the contacture tree from both the question and the step back question. Here's our final answer. So again, this is kind of a nice technique. Probably depends on a lot of the types of like the type of domain you want to perform retrieval on. But in some domains where, for example, there's a lot of kind of conceptual knowledge that underpins questions you expect users to ask, this step back approach could be really convenient to automatically formulate a higher level question to, for example, try to improve retrieval. I can imagine if you're working with I kind of textbooks or like technical documentation where you've made independent chapters focused on more high level kind of like concepts and then other chapters on like more detailed like implementations, this kind of like step back approach and independent retrieval could be really helpful. Thanks. This is Lancer and Lanchine. This is the fifth video focused on queer translation in our Rag from scratch series. We're going to be talking about a technique called hide. So again, queer translation sits kind of at the front of the overall rag flow. And the objective is to take an input question and translate it in some way that improves retrieval. Now, hide is an interesting approach that takes advantage of a very simple idea. The basic rag flow takes a question and embeds it, takes a document and embeds it and looks for similarity between an embedded document and embedded question. But questions and documents are very different text objects. So documents can be like very large chunks taken from dense publications or other sources, whereas questions are short kind of terse potentially ill-worded from users. And the intuition behind hide is take questions and map them into document space using a hypothetical document or by generating a hypothetical document. That's the basic intuition. And the idea kind of shown here visually is that in principle for certain cases, a hypothetical document is closer to a desired document you actually want to retrieve in this, you know, high dimensional embedding space than the sparse rot input question itself. So again, it's just kind of means of translating raw questions into these hypothetical documents that are then better suited for retrieval. So let's actually do a code walk through to see how this works. And it's actually pretty easy to implement, which is really nice. So first we're just starting with a prompt and we're using the same notebook that we've used for prior videos. We have a blog post on agents already indexed. So what we're going to do is define a to generate a hypothetical documents. In this case, we'll say write a paper passage to answer a given question. So let's just run this and see what happens. Again, we're taking our prompt, piping it to open AI, check GPD, and then using string op-appariser. And so here's a hypothetical document section related to our question. And this is derived of course, Mel and I'm just kind of embedded kind of world knowledge, which is a same place to generate hypothetical documents. Now, let's now take that hypothetical document. And basically, we're going to pipe that into retriever. So this means we're going to fetch documents from our index related to this hypothetical document that's been embedded. And you can see we get a few retrieved chunks that are related to this hypothetical document. That's all we've done. And then let's take the final step where we take those retrieved documents here, which we defined. And our question, we're going to pipe that into this rag prompt. And then we're going to run our kind of rag chain right here, which you've seen before. And we get our answer. So that's really it. We can go to Lang's method, we can actually look at what happened. So here, for example, this was our final rag prompt. And so the final question based on this context. And here is the retrieved document that we passed in. So that parts kind of straightforward. We can also look at, okay, this is our retrieval. Okay, no, this is actually what we generated a hypothetical document here. Okay, so this is our hypothetical document. So we've run chat open AI, we generate this passage, was our hypothetical document, and then we've run retrieval here. So this is basically showing hypothetical document generation followed by retrieval. So again, here was our passage, if we passed in. And then here's our retrieved documents from the tree for which are related to the passage content. So again, in this particular index case, it's possible that the input question was sufficient to retrieve these documents. In fact, given prior examples, I know that some of these same documents are indeed retrieved just from the wrong question, but in other contexts that may not be the case. So folks have reported nice performance using hide for certain domains. And the really convenient thing is that you can take this document generation prompt, you can tune this arbitrarily for your domain of interest. So it's absolutely worth experimenting with. It's a neat approach that can overcome some of the challenges with retrieval. Thanks very much. This is LanSumLangChain. This is the 10th video in our Rack from Scratch for Thierry's, focused on routing. So we talk through query translation, which is the process of taking a question and translating in some way. It could be decomposing it, using step back prompting or otherwise. But the idea here was take our question, change into a form that's better suited for retrieval. Now, routing is the next step, which is basically routing that potentially decomposed question to the right source. And in many cases, that could be a different database. So let's say in this toy example, we have a vector store relational DB and a graph DB, that what we redo with routing is we simply route the question based upon the content, the question to the relevant data source. So there's a few different ways to do that. One is what we call logical routing. In this case, we basically give an LLM knowledge of the various data sources that we have at our disposal. And we let the LLM kind of reason about which one to apply the question to. So it's kind of like the LLM is applying some logic to determine which data source, for example, to use, alternatively, new semantic routing, which will take a question, we embed it. And for example, we embed prompts. We then compute the similarity between our question and those prompts. And then we choose a prompt based upon the similarity. So the general idea is in our diagram, we talk about routing to, for example, a different database, but it can be a general, it can be routing to different prompts. It can be really arbitrarily taking this question and sending it different places, be it different prompts, be it different vector stores. So let's walk through the code a little bit. So you can see just like before, we've done a few pip installs, we've set up length Smith. And let's talk through logical routing first. So in this toy example, let's say we had, for example, three different docs, like we had Python docs, we had JS docs, we had GoLang docs. And we want to do is take a question route it to one of those three. So what we're actually doing is we're setting up a data model, which is basically going to be bound to our LLM and allow the LLM to output one of these three options as a structured object. So you really think of it as like classification, classification plus function calling to produce a structured output, which can strengthen these three possibilities. So the way we do that is, let's just zoom in here a little bit. We can define like a structured object we want to get out from our LLM, like in this case, we want, for example, you know, one of these three data sources to be output. We can take this and we can actually convert it into open, like open an AI, for example, function schema. And then we actually pass that in and bind it to our LLM. So what happens is we ask a question, our LLM invokes this function on the output to produce an output that adheres to the schema that we specify. So in this case, for example, we output like, you know, in this toy example, let's say we want to like, you know, an output to be data source vector store or SQL database. The output will contain a data source object and it'll be, you know, one of the options we specify as a JSON string. We also instantiate a parser from this object to parse that JSON string to an output like a pedantic object, for example, such as one toy example. And let's show one up here. So in this case, again, we had our three doc sources. We bind that to our LLM, so you can see we do with structured output, basically under the hood, that's taking that object definition, turning into function schema and binding that function schema to our LLM. And we call our prompt, you're an expert routing user question based on, you know, programming language that user referring to. So let's define our router here. Now what we're going to do is we'll ask a question that is Python code. So we'll call that and now it's done. And you see the object we get out is indeed, it's a route query object. So it's exactly in here is this data model we've set up. And in this case, it's correct. So it's calling this Python docs. So you can extract that right here as a string. Now once we have this, you can really easily set up like a route. So this could be like our full chain where we take this router, which you define here. And then this choose route function can basically take that output and do something with it. So for example, if Python docs, this could then apply the question to like a retriever full of Python information or JS saying thing. So this is where you would hook basically that question up to different chains that are like, you know, retriever chain one for Python, retriever chain two for JS and so forth. So this is kind of like the routing mechanism. But this is really doing the heavy lifting of taking an input question and turning it into a structured object that restricts the output to one of a few output types. So we care about in our like routing problem. So that's really kind of the way this all hooks together. Now semantic routing is actually maybe a little bit more straightforward based on what we've seen previously. So in that case, let's say we have two prompts with a physics prompt, we have a math prompt. We can embed those prompts no problem. We do that here. Now let's say we have an input question from a user like in this case, what is the black hole? We pass that through. We then apply this runable lambda function, which is defined right here. What we're doing here is we're embedding the question. We're computing similarity between the question and the prompts. We're taking the most similar. And then we're basically choosing the prompt based on that similarity. And you can see us run that and try it out. And we're using the physics prompt. And there we go. Black holes region in space. So that just shows you kind of how you can use semantic routing to basically embed a question and then for example various prompts, pick the prompt based on semantic similarity. So that really gives you just two ways to do routing. One is logical routing with function calling. It can be used very generally. In this case, we applied it to like different coding languages. But imagine these can be swapped out for like, you know, my Python, my like vector store versus my graph DB versus my relational DB. And you could just very simply have some description of what each is. And you know, then not only will the lm do reasoning, but it'll also return an object that can be parsed very cleanly to produce like one of a few very specific types, which then you can reason over like we did here in your routing function. So that kind of gives you the general idea. And these are really very useful tools. And I encourage you to experiment with them. Thanks. This is my some language. This is the eleventh part of our RAD from scratch video series focused on query construction. So we previously talked through query translation, which is the process of taking a question and converting it or translating it into a question that's better optimized for retrieval. Then we talked about routing, which is the process of going taking that question, routing it to the right source, be it a given vector store, graph DB, or SQL DB, for example. Now we're going to talk about the process of query construction, which is basically taking natural language, you can verdict it into particular domain specific language for one of these sources. Now we're going to talk specifically about the process of going from natural language to a metadata filters for vector stores. The problem statement is basically this. Let's imagine we had an index of chain video transcripts. You might want to ask a question, give me, you know, or find find me videos on chat, chain published after 2024, for example. The the process of query structuring basically converts this natural language question into a structured query that can be applied to the metadata filters on your vector store. So most vector stores will have some kind of metadata filters that can do structure querying on top of the chunks that are indexed. So for example, this type of query will retrieve all chunks that talk about the topic of chatlang chain, published after the date 2024. That's kind of the problem statement. And to do this, we're going to use function calling. In this case, you can use, for example, open AI or other providers to do that. And what we're going to do is at a high level, take the metadata fields that are present in our vector store and provide them to the model as kind of information. And the model then can take those and produce queries that adhere to the schema provided. And then we can parse those out to a structured object like a pedantic object, which again, which can then be used in search. So that's kind of problem statement. Let's actually walk through code. So here's our notebook, which we kind of gone through previously. And I'll just show you as an example. Let's take an example YouTube video and let's look at the metadata that you get with the transcript. So you can see you get stuff like description, URL, yeah, publish date, like things like that. Now let's say we did an index that had, um, basically had a number of different metadata fields and filters that allowed us to do range filter in our like view, account publication date, the video length, or unstructured search on contents and title. So those are kind of like the, imagine we had an index that had those kind of filters available to us. What we can do is capture that information about the available filters in an object. So we're calling that at this tutorial search object, kind of encapsulates that information about the available searches that we can do. And so we basically enumerated here, content search and title search or semantic searches that can be done over those fields. Then these filters then are various types of structured searches we can do on like the length, the view count, and so forth. And so we can just kind of build that object. Now we can set this up really easily with a basic simple prompt that says, you know, your next work in for natural language into database queries, you've access to the database tutorial videos, um, given a question, return a database query optimized for retrieval. So that's kind of it. Now here's the key point though. When you call this LLM with structured output, you're binding this pedantic object which contains all the information about our index to the LLM, which is exactly what we talked about previously. It's really this process right here. You're taking this object, you're converting it into a function schema, for example, a AI, you're binding that to your model, and then you're going to be able to get structured object out versus JSON string from a natural language question, which can then be parsed into a pedantic object, which you get out. So that's really the flow. It's taking advantage of function calling as we said. So if we go back down, we set up our query analyzer chain right here. Now let's try to run that just on a purely semantic input. So rack from scratch, let's run that. And you can see this just does like a content search and a title search. That's exactly what we would expect. Now if we pass a question that includes a date filter, let's just see if that would work. And there we go. So you kind of still get that semantic search. But you also get search over, for example, published date, earliest and latest published date kind of as you would expect. Let's try another one here. So videos focus on the topic of chat language and they're published from work 2024. This is just kind of rewrite of this question in a slightly different way using a different date filter. And then you can see we can get we get content search, title search, and then we can get kind of a date search. So this is a very general strategy that can be applied kind of broadly to different kinds of querying you want to do. It's really the process of going from an unstructured input to a structured query object out following an arbitrary schema that you provide. And so as noted, really this whole thing we created here, this tutorial search is based upon the specifics of our vector store of interest. And if you want to learn more about this, I linked to some documentation here that talks a lot about different types of integrations. We have a different vector store providers to do exactly this. So it's a very useful trick. It allows you to do kind of query, it's a metadata filter filtering on the fly from a natural language question. It's a very convenient trick that works with many different vector DBs. So encourage you to play with it. Thanks. This is last from LINE chain. I'm going to talk about indexing and multi-refermentation indexing in particular, but the 12th part of our rag from scratch series here. So we previously talked about a few different major areas. We talk about query, query translation, which takes a question and translates it in some way to optimize for a treeble. We talk about routing, which is the process of taking a question, routing it to the right data source, be it a vector store, graph DB, SQL DB. We talk about query construction. We dug into basically query construction for vector stores, but of course there's also text SQL, text decipher. So now we're going to talk about indexing a bit. In particular, we're going to talk about indexing techniques for vector stores. And I want to highlight one particular method today called multi-representation indexing. So the high level idea here is derived a bit from a paper called proposition indexing, which kind of makes a simple observation. You can think about decoupling, raw documents, and the units you use for a treeble. So in the typical case, you take a document, you split it up in some way to index it, and then you embed the split directly. This paper talks about actually taking a document, splitting it in some way, but then using an LLEM to produce what they call a proposition, which you can think of as a distillation of that split. So it's using an LLEM to modify that split in some way to distill it or make it a crisper, like summary, so to speak, that's better optimized for a treeble. So that's one piece of intuition. So we've actually taken that idea, and we've kind of built on it a bit in a really nice way, that I think is very well suited actually for long context LLEMs. So the idea is pretty simple. You take a document, and you actually distill it or create a proposition like they show in the prior paper, I kind of typically think of this as just produces summary of the document, and you embed that summary. So that summary is meant to be optimized for a treeble, so it might contain a bunch of keywords from the document or like the big ideas, such that when you embed the summary, you embed a question, you do search, you basically can find that document based on this highly optimized summary for a treeble. So that's kind of represented here in your vector store, but here's the catch. You independently store the raw document in a doc store, and when you basically retrieve the summary in the vector store, you return the full document for the LLEM to perform generation. And this is a nice trick because at generation time, now with long context LLEM, for example, the LLEM can handle that entire document. You don't need to worry about splitting it or anything. You just simply use this summary to create a really nice representation for fishing out that full doc, use that full doc in generation. There might be a lot of reasons you want to do that. You want to make sure that LLEM has the full context to actually answer the question. So that's the big idea. It's a nice trick. And let's walk through some code here. Let's give a notebook all set up, just like before. We've got some pip installs, set to make BI keys here for Langsmith. Kind of here's a diagram. Now let me show an example. Let's just load two different blog posts, ones about agents, one is about human data quality. And what we're going to do is let's create a summary of each of those. So this is kind of the first step of that process where we're going from the raw documents to summaries. Let's just have a look and make sure those ran. So okay, cool. So the first doc discusses building autonomous agents. The second doc contains importance of high quality human data and training. Okay, so that's pretty nice. We have our summaries. Now we're going to go through process. That's pretty simple. First we define a vector store that's going to index those summaries. Now we're going to define what we call like our document storage. You're going to store the full documents. Okay, so this multi vector retriever kind of just pulls us two things together. We basically add our doc store. We add this byte store is basically the the full document store. The vector store is our vector store. And now this ID is we're going to use to reference between the chunks or the summaries and the full documents. That's really it. So now for every document, we'll define a new doc ID. And then we're basically going to like take our summary documents. And we're going to extract for each of our summaries. We're going to get this associated doc ID. So there we go. So let's go ahead and do that. So we have our summary docs, which we added in the vector store. We have our full documents, our doc IDs, and the full raw documents, which are added to our doc store. And then let's just do a query of vector store, like a similarity search on our vector store. So memory and agents, and we can see okay. So we can extract from the summaries. We can get, for example, the summary that pertains to agents. That's a good thing. Now let's go ahead and run a query, get relevant documents on our retriever, which basically combines the summaries, which we use for a tree pool. Then the doc store, which we use to get the full doc back. So we're going to apply our query. We're going to basically run this. And here's the key point. We've gotten back the entire article. And we can actually, if you want to look at the whole thing, we can just go ahead and do this. Here we go. So this is the entire article that we get back from that search. So it's pretty nice trick. Again, we query with just memory and agents. And we can kind of go back to our diagram here. We created for memory and agents. It's stashed our summaries. It found the summary related to memory and agents. I use the doc ID to reference between the vector store and the doc store. It fishes out the right full doc. Returns us the full document. In this case, the full web page. That's really it. Simple idea. Nice way to go from basically like nice simple proposition style or summary style indexing to full document retrieval, which is very useful, especially with long context LMs. Thank you. This is Lance from Langton. This is the 13th part of our rag from scratch series focused on a technique called Raptor. So Raptor sits within kind of an array of different indexing techniques that can be applied on vector stores. We just talked about multi representation indexing. We I probably have a link to a video that's very good talking about the different means of chunking. So I encourage you to look at that. And we're going to talk today about a technique called Raptor, which you can kind of think about as a technique for hierarchical indexing. So the high level intuition is this. Some questions require very detailed information from a corpus to answer like pertain to a single document or single chunk. So like we can call those low level questions. Some questions require consolidation across kind of broad swast of a document still cross like many documents or many chunks within a document. They can call those like higher level questions. And so there's kind of this challenge in retrieval. And that typically we do like K nearest neighbors retrieval like we've been talking about. You're fishing out some number of chunks. But what if you have a question that requires information across like five six, you know, or a number of different chunks, which may exceed, you know, the K parameter in your retrieval. So again, we typically do retrieval. You might set a K parameter of three, which means you're retrieving three chunks from your vector store. And maybe you have a very high level question that could benefit from invocation across more than three. So this technique called Raptor is basically a way to build a hierarchical index of documents summaries. And the intuition is this. You start with a set of documents as your leafs here on the left. You cluster them and then you summarize each cluster. So each cluster of similar documents will consolidate information from across your context, which is, you know, your context could be a bunch of different splits or it could even be across a bunch of different documents. You're basically capturing similar ones and you're consulting the information across them in a summary. And here's the interesting thing. You do that recursively until you hit like a limit or you end up with one single cluster. That's the kind of very high level summary of all of your documents. And what the paper shows is that if you basically just collapse all these and index them together as a big pool, you end up with a really nice array of chunks that span the abstraction hierarchy. Like you have a bunch of chunks from individual documents that are just like more detailed chunks pertaining to that, you know, single document. But you also have chunks from these summaries or I would say like, you know, maybe not chunks, but in this case the summaries like a distillation. So, you know, rock chunks on the left that will represent your leafs are kind of like the raw form of information, either raw chunks or raw documents, and then you have these higher level summaries, which are all indexed together. So if you have higher level questions, they should basically be more similar in Symatic Search, for example, to these higher level summary chunks. If you have lower level questions, then they'll retrieve these the more lower level chunks. And so you have better Symantec coverage across like the abstraction hierarchy of question types. That's the intuition. They do a bunch of nice studies to show that this works pretty well. I actually did a deep dive video just on this, which I linked below. I did want to cover it briefly, just at a very high level. So let's actually just do kind of a code walkthrough. And I've added to this rack from scratch course notebook, but I link over to my deep dive video as well as the paper and the full code notebook, which is already checked in is discussed at more length in the deep dive. The technique is a little bit detailed, so I only want to give you very high levels, the kind of overview here. And you can look at the deep dive video if you want to go in more depth. Again, we talked through this abstraction hierarchy. I applied this to a large set of langshade documents. So this is me in loading, basically all of our langshade expression language docs. This is on the order of 30 documents. You can see I do a histogram here at the token counts for document. Some are pretty big, most are fairly small, less than 4,000 tokens. And what I did is I indexed all of them individually. So all those raw documents, you kind of imagine are here on the left. And then I do embedding, I do clustering, summarization, and I do that recursively until I end up with, in this case, I believe I only set like three levels of recursion. And then I save them all in my vector store. So that's like the high level idea. I'm applying this Ractor Technique to a whole bunch of langshade documents that have fairly large number of tokens. So I do that. And yeah, I use, actually I use both clod as well as open AI here. This talks through the clustering method, which they that they use, which is pretty interesting. You can kind of dig into that and own it if you're really interested. This is a lot of their code, which I cite accordingly. This is basically implementing the clustering method that they use. And this is just simply the document embedding stage. This is like basically embedding and clustering. That's really it. Some text formatting, summarizing of the clusters right here. And then this is just running that whole process recursively. That's really it. This is tree building. So basically I have the raw docs. Let's just go back and look at doc texts. So this should be all my raw documents. So that's right, you can see it here. Doc text is basically just the text and all those langshade documents that I pulled. And so I run this process on them right here. So this is the recursive embedding cluster. Basically runs and produces that tree. Here's the results. This is me just going through the results and basically adding the result text to this list of texts. Okay, so here's what I do. This leaf text is all the raw documents. And I'm appending to that all the summaries. That's all it's going on. And then I'm indexing them all together. That's the key point, rag chain. And there you have it. That's really all you do. So anyway, I encourage you to look at this in depth. It's a pretty interesting technique. It works well with long contexts. So for example, one of the arguments I made is that it's kind of a nice approach to consolidate information across like a span of large documents. Like in this particular case, my individual documents were line chain expression language docs, each being somewhere in the order of, you know, in this case, like, you know, most of them are less than 4,000 tokens. Some pretty big, but I index them all. I cluster them without any splits, embed them, cluster them, build this tree, and go from there. And it all works because we now have LLM that can go out to, you know, 100 or 200,000 up to a million tokens in context. So you can actually just do this process for big swaths of documents in place without any, without any splitting. It's a pretty nice approach. So I encourage you to think about, look at it. Watch the deep that video if you really want to go deeper on this. Um, thanks. This is Lance from LLM chain. This is the 14th part of our rag from scratch series. We're going to, I'm going to be talking about a approach called cold bear. Um, so we've talked about a few different approaches for indexing. And just as kind of a fresher indexing falls, uh, kind of right down here in our flow. We started initially with query translation, taking a question, translating it in some way to optimize retrieval. We talked about routing it to a particular database. We then talked about query construction. So going from natural language to the DSL or domain specific language for each any of the databases that you want to work with, those are, you know, metadata filters for vector stores or cipher for graph DB or SQL for relational DB. So that's kind of flow we talked about today. We talked about some indexing approaches like multi representation indexing. We gave a small shout out to Greg Cameron in the series on chunking. Uh, we talked about hierarchical in indexing. I want to include one advanced kind of embedding approach. So we talked a lot about embeddings are obviously very central to semantic similarity search, um, and retrieval. So one of the interesting points that's been brought up is that embedding models, of course, take a document. You can see here on the top and embed it basically compress it to a vector. So it's kind of a compression process. You're representing all the semantics of that document in a single vector. You're doing the same to your question. You're doing similarity search between the question and embedding and the document embedding, um, in order to perform retrieval. You're typically taking the, you know, K most similar, um, document embedding is given a question. And that's really how you're doing it. Now a lot of people said, well, hey, compressing a full document with all these nuance to a single vector seems a little bit, um, overly restrictive, right? And this is a fair question to ask. Um, there's been some interesting approaches to try to address that. And one is this, this, this approach method called colbert. So the intuition is actually pretty straightforward. There's a bunch of good articles I link down here. This is my little cartoon to explain it, which I think is hopefully kind of helpful. But here's the main idea. Instead of just taking a document and compressing it down to a single vector basically, a single, uh, what we might call an embedding vector, we take the document, we break it up into tokens. So tokens are just like, you know, units of, of content. It depends on the token, are you used? We talked about this earlier. So you basically tokenize it. And you produce basically an embedding or a vector for every token. And there's some kind of positional, uh, weighting that occurs when you do this process. So obviously you look to look at the implementation numbers in the details. But the intuition is that you're producing some kind of representation for every token. Okay. Are you doing the same thing for your questions? So you're taking your question, you're breaking into a tokens and you have some representation or vector per token. And that what you're doing is for every token in the question, you're computing the similarity across all the tokens in the document. And you're finding the max. You're taking the max, you're storing that, and you're doing that process for all the tokens in the question. So again, token two, you compare it to every token in the in the document, compute the max. And then the final score is in this case, the sum of the max similarities between every question token and any document token. So it's an interesting approach. Uh, it reports very strong performance. Latency is definitely a question. Um, so kind of production readiness is something you should look into. But it's a, it's a process worth mentioning here, because it's pretty interesting. Um, and let's walk through the code. So there's actually a really nice library called Racketooie, which makes it very easy to play with Colbert. Um, she's pep install out here. I've already done that. And yet we can use one of their pre-trained models to mitigate this process. So I'm basically following their documentation. This is kind of what they recommended. Um, so I'm running this now. Hopefully this runs so much quickly. I'm not sure. I, I previously have loaded this model. So hopefully it won't take too long. And if, yeah, you can see it's pretty quick. Uh, I'm on a Mac M2 with 32 gigs. Um, so just as like a context in terms of my system. Um, this is from their documentation. We're just grabbing a Wikipedia page. This is getting a full document on Miyazaki. So that's cool. We're going to grab that. Now this is just from their docs. This is basically how we create an index. So we provide the, you know, some index name, the collection, um, the dot max document length. And yeah, you should look at their documentation with these slides. These are the defaults. So I'm going to create my index. Um, so they get some logging here. So it gets it's working under the hood. Um, and by the way, I actually have their documentation open. So you can kind of follow along. Um, so, um, let's see. Yeah, right about here. So you can kind of follow this indexing process. Create an index. So you can load a train, uh, a train to model. This can be either your own pre-train model or one of our Shana hub. Um, and this is kind of the process we're doing right now. Create index is just a few lines of code. And it's exactly what we're doing. Um, so this is the, you know, my documents. And this is the indexing step that we just, we just kind of walk through. And it looks like it's done. Um, so you get a bunch of logging here. That's fine. Um, they'll actually see if this works. So we're going to run Rack search. What an emotion studio did means. I've got you found that our K parameter and we get some results. Okay. So it's running and cool. We get some documents out. So, you know, it seems to work. Now what's nice is you can run this within line chain is a line chain retriever. So that basically wraps this as a line chain retriever. And then you can use it freely as a retriever within line chain. It works with all the other different LMs and all of all their components like re-rankers and so forth that we talk through. So you can use this directly as a retriever. Let's try this out. And boom, nice and fast. Um, and we get our documents again. This is a super simple test example. You should run this maybe on more complex cases, but it's pretty easy. It's been up. It's a really interesting alternative indexing approach. Um, using again, like we talked through, um, a very different algorithm for computing docs similarity that may work better. I think an interesting regime to consider this would be longer documents. So if you want like longer, um, yeah, if you basically want kind of long context embedding, I think you should look into, for example, the, uh, max token limits for this approach because it partitions the document into into each token. I would be curious to dig into kind of what the overall context limits are for this approach of Colbert, but it's really to consider and their port very strong performance. So again, I encourage you to play with it and this is just kind of an intro to how to get set up and to start experimenting with it really quickly. Thanks. Uh, this is last from Lanchein. I'm going to be talking about using the Landrack to build a diverse and sophisticated rag flows. So just to set the stage, the basic rag flow you can see here starts with a question retrieval of relevant documents from an index, which are passed into the context window of NLLN for generation of an answer and ground in the dream documents. That's kind of the basic outline. And we can see it's like a very linear path. Um, in practice though, you often encounter a few different types of questions like when do we actually want to retrieve based upon the context of the question? Um, are the retrieve documents actually good or not? And if they're not good, can we discard them? And then how do we loop back and retry retrieval with, for example, an improved question? So these types of questions motivate an idea of active rag, which is a process where an LLM actually decides when and where to retrieve based upon like existing retrievals or existing generations. Now, when you think about this, there's a few different levels of control that you have over an LLN in a rag application. The base case like we saw with our chain is you just use an LLNM to choose a single steps output. So for example, in traditional rag, you feed it documents and it decides to generation. So it's just kind of one step. Now, a lot of rag workflows will use the idea of routing. So like given a question, could I route it to a vector store or a graph DB? Um, and we have seen this quite a bit. Now, this new idea that I want to introduce is how do we build more sophisticated logical flows in a rag pipeline that you let the LLM choose between different steps, but specify all the transitions that are available. And this is known as we call it state machine. Now, there's a few different architectures that have emerged to build different types of rag chains. And of course, chains are traditionally used just for like very basic rag. But this notion of a state machine is a bit newer. And LLNGRAPH, which you recently released, provides a really nice way to build state machines for rag and for other things. And the general idea here is that you can lay out more diverse and complicated rag flows and then implement them as graphs. And it kind of motivates this more broad idea of like flow engineering and thinking through the actual like workflow that you want and then implementing it. And we're going to actually do that right now. So I'm going to pay a recent paper called CRAG, corrective rag, which is really a nice method for active rag. Then incorporates a few different ideas. So first you retrieve documents and then you grade them. Now, if at least one document exceeds the threshold for relevance, you go to generation to generate your answer. And it does this knowledge or five minutes stage after that. But let's not worry about that for right now. It's kind of not essential for understanding the basic flow here. So again, you do grade for relevance for every document. If any is relevant, you generate. Now, if they're all ambiguous or incorrect based on your grader, you retrieve from an external source. They use web search. And then they pass that as their context for answer generation. So it's a really neat workflow where you're doing retrieval just like with basic rag, but then you're reasoning about the documents. If they're relevant, go ahead and at least one is relevant. Go ahead and generate if they're not retrieved from alternative source. And then pack that into the context and generate your answer. So let's see how we would implement this as a state machine using LineGraph. We'll make a few simple locations. We're going to first decide if any documents are relevant. We'll go ahead and do the web search to supplement the output. So that's just a kind of one minor modification. We'll use tably search for web search. We use query writing to optimize the search, but it follows a lot of the intuitions of the main paper. Small note here, we set the tably API key. And another small note, I've already set my length API key, which we'll see is useful that later for observing the resulting traces. Now, I'm going to index three blog posts that I like. I'm going to use ChromedyD, I'm going to use OpenI embeddings. I'm going to run this right now. This will create a vector store for me from these three blog posts. And then what I'm going to do is define a state. Now this is kind of a core object that's going to be passed around my graph that I'm going to modify. And right here is where to find it. And the key point to note right now is it's just a dictionary. And as we contain things that are relevant for RAC, like question documents generation. And we'll see how we up with that in the little bit. But the first thing to note is we define a state and this is what's going to be modified in every note of our graph. Now here's really the crux of it. And this is the thing I want to zoom in on a little bit. So when you kind of move from just thinking about prompts to thinking about overall flows, it's like kind of a fun and interesting exercise. I kind of think about this as it's been mentioned on Twitter a little bit more like flow engineering. So let's think through what was actually done in the paper. And what modifications to our state are going to happen in each stage. So we start with a question. You can see that on the far left. And this kind of state is represented as a dictionary like we have. We start with a question. We perform a treeval from our vector store, which we just created. That's going to give us documents. So that's one node. We made an adjustment to our state by adding documents. That's step one. Now we have a second node. We're going to grade the documents. And in this node, we might filter some out. So we are making a modification to state, which is why it's a node. So we're going to have a greater. Then we're going to have what we're going to call a conditional edge. So we saw we went from question to retrieval. Retrieval always goes to grading. And now we have a decision. If any document is irrelevant, we're going to go ahead and do web search to supplement. And if they're all relevant, we'll go to generation. It's a minor kind of a minor kind of logical decision that we're going to make. If any or not relevant will transform the query and we'll do web search and we'll use that for generation. So that's really it. And that's how we can think about our flow and how our states can be modified throughout this flow. Now, all we then need to do and I kind of found spending 10 minutes thinking carefully through your flow engineering is really valuable because from here it's really just implementation details. And it's pretty easy as you'll see. So basically, I'm going to run this code block but then we can like walk through some of it. I won't show you everything. So get a little bit boring. But really all we're doing is we're finding functions for every node that take in the state and modify it in some way. That's all it's going on. So think about retrieval. We run retrieval. We take in state. Remember it's a DICT. We get our state DICT like this. We extract one key question from our DICT. We pass that to our retriever. We get documents and we write back out state now with documents key added. That's all. Generates going to be similar. We take in state. Now we have our question and documents. We pull in a prompt. We define an LM. We do minor post-processing on documents. We set up a chain for retrieval or sorry for generation which is just going to be take our prompt. Pump from that to an LM. Pass the output to a string. And we run it right here invoking our documents and our question to get our answer. We write that back to state. That's it. And you can kind of follow here for every node. We just define a function that performs the state modification that we want to do on that node. Grading documents is going to be the same. In this case, I do a little thing extra here because I actually define a gigantic data model for my grader so that the output of that particular grading chain is a binary yes or no. You can look at the code. I'll make sure it's all shared. And that just makes sure that our output is very deterministic. So that we then can down here perform logical filtering. So what you can see here is we define this search value. No. And we iterate through our documents. We grade them. If any document is graded as not relevant, we flag this search thing to yes. That means we're going to perform web search. We then add that to our state dictate at the end. So we run web search. Now that values true. That's it. And you can kind of see we go through some other nodes here as a web search node. Now here is where our one conditional edge we define right here. This is where we decide to generate or not based on that search key. So we again get our state. Let's extract the various values. So we have this search value now. If search is yes, we return the next node that we want to go to. So in this case, it'll be transform query, which will then go to web search else we go to generate. So what we can see is we laid out our graph, which you can kind of see up here. And now we define functions for all those nodes as well as the conditional edge. And now we scroll down. All we have to do is just lay that out here again as our flow. And this is kind of what you might think I was like kind of flow engineering where you're just laying out the graph as you drew it where we have set our entry point as retrieve. We're adding an edge between retrieve and grade documents. So we went retrieval, grade documents. We add our conditional edge depending on the grade, either transform the query, go to web search or just go to generate. We create an edge between transform query and web search and web search to generate. And then we also have an edge generate to end. And that's a whole graph. That's it. So we can just run this. And now I'm going to ask a question. So let's just say how does agents memory work, for example, let's just try that. And what this is going to do is going to print out what's going on as we run through this graph. So first we can see output from retrieve. This is going to be all of our documents that we were retrieved. So that's that's fine. This is from our retriever. Then you can see that we're doing a relevance check across our documents. And this is kind of interesting. You can see we're grading them here. One is great is not relevant. And you can see the documents are now filtered because we remove the one that's not relevant. And because one is not relevant, we decide, okay, we're going to transform the query in run web search. And you can see after query transformation, we rewrite the question slightly. We then run web search. And you can see from web search, it searched from some additional sources. Which you can actually see here. It's appended as a. So here it is. So here it's a new document appended from web search, which is from memory knowledge requirements. So it may seem looked up some AI architecture related memory web results. So that's fine. That's exactly what we want to do. And then we generate a response. So that's great. And this is just showing you everything in kind of gory detail. But I'm going to show you one other thing that's that's really nice about this. If I go to Lang Smith, I have my API I key sets. So all my generations are just logged to Lang Smith. And I can see my line graph run here. Now what's really cool is this shows me all of my nodes. So remember, we had retrieve grade. We evaluated the grade because one was irrelevant. We then went ahead and transform the query. We did a web search. We've pended that to our pontex. You can see all those steps are laid out here. In fact, you can even look at every single greater and its output. I will move this up slightly. So you can see the different scores or grades. Okay, so this particular retrieval was graded as not relevant. So that's fine. That can happen in some cases. And because of that, we do a query transformation. So we modified the question slightly. How does memory, how does the memory system and artificial agents function? So it's just a minor, rephrasing the question. We did this heavily web search. This is where it queried from this particular block post or medium. So it's like a same web query. We can like sanity check it. And then what's neat is we can go to our generate step, look at open AI. And here's our full prompt. How does the memory system in our artificial agents function? And then here's all of our documents. So this is the web search as well as we still have the relevant chunks that were retrieved from our blog posts. And then here's our answer. So that's really it. You can see how really moving from the notion of just like I should go back to the original. Moving from I will try to open this up a little bit. I can see my face still. The transition from laying out simple chains to flows is a really interesting and helpful way of thinking about why grass really interesting because you can encode more sophisticated logical reasoning workflows. But in a very clean and well engineered way where you can specify all the transitions that you actually want to have executed. And I actually find this way of thinking and building kind of logical like workflows really intuitive. So the blog post coming out tomorrow that discusses both implementing self rag as well as see rag for two different active rag approaches using this idea of state machines and land graph. So I encourage you to play with it. I found it really intuitive to work with. I also found inspection of traces to be quite intuitive using land graph because every node is enumerated pretty clearly for you, which is not always the case when using other types of more complex reasoning approaches, for example, like agents. In any case, I hope this was helpful. And I definitely encourage you to check out kind of this notion of like flow engineering using land graph and in the context of rag can be really powerful. Hopefully as you see here. Thank you. Hey, this is Lance from Langshan. I want to talk to a recent paper that I saw called adaptive rag, which brings together some interesting ideas that have kind of been covered in other videos. So this actually ties them all together in kind of a fun way. So the two big ideas to talk about here are one of query analysis. So we've actually done kind of a whole rag from scratch series that walks to reach these things in detail. But this is a very nice example of how this comes together with some other ideas we've been talking about. So query analysis typically the process of taking an input question and modifying in some way to better optimize retrieval. There's a bunch of different methods for this. It could be decomposing it into sub questions. It could be using some clever techniques like step back prompting. But that's kind of like the first stage of query analysis. Then typically you can do routing. So you route a question to one of multiple potential sources. It could be one or two different vector stores. It could be relational DB versus a vector store. It could be web search. It could just be like an LLM fallback. So this is like one kind of big idea query analysis. Right. It's kind of like the front end of your rag pipeline. It's taking your question. It's modifying it in some way. It's sending it to the right place. Be it a web search, be it a vector store, be it a relational DB. So that's kind of topic one. Now topic two. It's like that's been brought up in a few other videos of what I kind of call flow engineering or adaptive rag, which is the idea of doing tests in your rag pipeline or in your rag inference flow to do things like check relevant documents. Check whether or not the answer contains hallucinations. So this recent blog post from Himalhu saying actually covers evaluation in some really nice detail. And one thing he highlighted explicitly is actually this topic. So he talks about unit tests. And in particular, he says something really interesting here. He says, you know, unlike typical unit tests, you want to organize these assertions in places beyond typical unit testing such as data cleaning. And here's the key point automatic retries during model inference. That's the key thing I want to draw your attention to. It's a really nice approach. We've talked about some other papers that do that, like corrective rag, self rag. But it's also cool to see it here and kind of encapsulated in this way. The main idea is that you're using kind of unit tests in your flow to make corrections like if your retrieval is bad, you can correct from that. If you're generation of hallucinations, you can correct from that. So I'm going to kind of draw out like a cartoon diagram of what we're going to do here. And you can kind of see it here. We're starting the question. We talk about queer analysis. We're going to take our question. And we're going to decide where it needs to go. And for this particular toy example, I'm going to say either send it to a vector store, send it to web search or just have the LM answer it. Right. So that's the kind of my fallback behavior. Then we're going to bring in that idea of kind of online flow engineering or unit testing where I'm going to have my retrieval, either from the vector store or web search. I'm then going to ask, is this actually relevant to the question? If it isn't, I'm actually going to kick back to web search. So it's a little bit more relevant in the case if I've routed to the vector store, done retrieval, documents aren't relevant. I'll have a fallback mechanism. Then I'm going to generate a check for hallucinations in my generation. And then I check for for whether or not the generation actually answers the question. Then I return my answer. So again, we're tying together two ideas. One is queer analysis, like basically taking a question, routing it to the right place, modifying it as needed. And then kind of online unit testing and iterative flow feedback. So to do this, I've actually heard a lot of people talk online about command R, a new model release from go here. It has some pretty nice properties that I was kind of reading about recently. So one, it has nice support for tool use and it does support query writing in the context of tool use. So this all rolls up in really nice capabilities for routing. It's kind of one. Now two, it's small 35 billion parameter. It's actually open weights. You can actually run this locally and I've tried that we can we can talk about that later. So it's also fast, sir, be a API. So it's kind of a small model. And it's well tuned for rags. I actually heard a lot of people talk about using co here for rag and it has a large context, 120,000 tokens. So it's like a nice combination of properties. It supports tools and routing. It's small and fast. So it's like quick for grading and it's well tuned for rag. So it's actually a really nice fit for this particular workflow where I want to do query analysis and routing. And I want to do kind of online checking and rag. So kind of there you go. Now let's just get to the coding bit. So I have a note book, kind of like usual. I've done a few pip installs. You can see it's nothing exotic. I'm bringing Langshanko here. I set my co here API key. Now I'm just going to set this Langshank project within Langsmith. So all my traces for this go to that project. And I have enabled tracing. So I'm using Langsmith here. So we're going to walk this flow. Let's do the first thing. Let's just build a vector store. So I'm going to build a vector store using co here embeddings with Chroma open source vector DB runs locally from three different web pages on blog posts that I like. So it pertains to agents prompt engineering and adversarial attacks. So now I have a retriever. I can run retriever in bulk. And I can ask a question about, you know, agent memory. And there we go. So we get documents back. So there we go. We have a retriever now. Now here's where I'm going to bring in co here. I also want to router. See, look at our flow. The first step is this routing stage, right. So what I'm going to do is, I'm going to find two tools, a web search tool and a vector store tool. And my preamble is just going to say your next part route is your questions to vector store web search. Now here's the key. I tell it what the vector store has. So again, my index, my vector store has agents prompt engineering adversarial attacks. I just repeat that here agents prompt and adversarial attacks. So it knows what's in the vector store. So use it for questions on these topics. Otherwise use web search. So that's it. I use command R here. Now I'm going to bind these tools to the model and a test of preamble and I have a structured LLM router. So let's give it a let's give this a few tests just to like kind of sandbox this a little bit. So I can invite here's my chain of a router prompt. I passed that to the structured LLM router, which I defined right here. And let's ask a few different questions like who will the bears draft and then a fell draft with a type of agent memory and hi, how are you. So I'm going to kick that off. And you can see you know it does web search. It does it goes the vector store and then actually returns this false. So that's kind of interesting. This is actually just saying if it does not use either tool. So for that particular query web search or the vector store was inappropriate, it'll just say, hey, I didn't call one of those tools. So that's interesting. We'll use that later. So that's my router tool. Now the second thing is my greater. And here's where I want to show you something really nice that is very general useful for many different prime problems you might encounter. So here's what I'm doing. I'm defining a data model for my great. So basically great documents. It's going to be a visit a pedantic object. It is basically a binary score here. Field specified here documents are relevant to the question. Yes, no. I have a preamble. You're a greater assessing relevance retrieve documents to use your question. Love. So you know, and then basically give it a good buying score. Yes, no. I'm using go ahead command R. But here's the catch. I'm using this with structured outputs thing and I'm passing my great documents data model to that. So this is the key thing we can test this right now as well. It's going to return an object based on the schema. I give it, which is extremely useful for all sorts of use cases. And let's actually zoom back up. So we're actually right here. So this greater stage, right. I want to constrain the output to yes, no. I don't want any preambles. I want anything because the logic I'm going to build in this graph is going to require a yes, no binary response from this particular edge in our graph. So that's why this greater tool is really useful. And I'll be asking like a mock question types of agent memory. I do a retriever. I do retrieval from our vector store. I get the talk and I test it. I invoke our greater retrieval greater chain with the question, the doc text and it's relevant as we would expect. So that's good. But again, let's just kind of look at that a little bit more closely. What's actually happening under the hood here. Here's the path and the object we passed. Here's a document in question and providing. Basically, it's converting this object into cohere function schema. It's binding that to the LLM. We pass in the document question. It returns an object based on string per our pedantic schema. That's it. And then it's just going to like parse that into a pedantic object, which we get at the end of the day. So that's what's happening under the hood with this with structured output thing. But it's extremely useful in your sea. We're going to use that a few different places because we want to ensure that in our in our flow here, we have three different grading steps in each time we want to constrain the output to yes, no. We're going to use that structured output more than once. This is my generation. So this is good old rag. Let's just make sure that works. I'm using rag chain typical rag prompt again. I'm using cohere for rag. Pretty easy. And yeah, so the rag piece works. That's totally fine. Nothing to it crazy there. I'm going to find this LLM fallback. So this is basically if you saw a router chain. If it doesn't use a tool, I want to fall back and just fall back to the LLM. So I'm going to kind of build that as a little chain here. So, okay, this is just a fallback. I have my preamble, just your your assistant answer the question based on your eternal knowledge. So again, that fallback behavior is what we have here. So what we've done already is we defined our router piece. We've defined our basic retrieval, our vector store we already have here. We defined our first logic or like grade check and we defined our fallback and we're just kind of roll through the parts of our graph and define these piece. So I'm going to have two other graders and they're going to use the same thing we just talked about, slightly different data model. I mean, same output, but actually just slightly different prompt. And you know, description, this in this case is the answers ground in the facts. Yes, no, this is my hallucination greater. And then I have an answer greater as well. And I've also run a test on each one and you can see I'm getting binary this this these objects out have a binary score. So this is a dynamic object with a binary score. And that's exactly what we want. Cool. And I have a search tool. So that's really nice. We've actually gone through and we've kind of laid out. I have like a router. I've tested it. We have a vector store. We've tested. We've tested each of our graders here. We've also tested generation of just doing rag. So we have a bunch of pieces built here. We have a fallback piece. We have web search. Now the question is how do I stick these together into this kind of flow. And for that, I like to use land graph. We'll talk a little bit about land graph versus agents a bit later. But I want to show you why this is really easy to do using land graph. So what's kind of nice is I've kind of laid out all my logic here. We've tested individually. And now all I'm going to do is I'm going to first layout the parts of my graph. So what you're going to notice here is first is a graph state. So this state represents kind of key parts of the graph are the key kind of objects within the graph. They're going to be modifying. So this is basically a graph centered around rag. We're going to question generation and documents. That's really kind of the main thing is we're going to be working with in a graph. So then you're going to see something that's pretty intuitive, I think. What you're going to see is we're going to basically walk through this flow. And for each of these little circles, we're just going to find a function. And these little squares or these these you can think about every circle as a node and every kind of diamond here as an edge or conditional edge. So that's actually what we're going to do right now. We're going to lay out all of our nodes and edges. And they're each one of them are just going to be a function. And you're going to see how we do that right now. So I'm going to go down here. I define my graph state. So this is what's going to be kind of modified and propagated throughout my graph. Now all I'm going to do is I'm just going to find a function for each of those nodes. So let me kind of go side by side and show you the diagram and like kind of show the nodes next to it. So here's the diagram. So we have a retrieve node so that kind of represents our vector store. We have a fallback node. That's this piece. We have a generate node. So that's basically going to do our rag. You can see there. We have a great documents node kind of right here. And we have a web search node. So that's right here. Cool. Now here's where we're actually going to find the edges. So you can see our edges are the places of the graph. They're kind of making different decisions. So this route question edge basic conditional edge is basically going to take an input question and decide where it needs to go. And that's all we're doing down here. It kind of falls when we did up at the top or we tested this individually. So recall we basically just invoke the question router returns our source. Remember if tool calls were not in the source, we do our fallback. So we show actually showed that all the way up here. Remember this if tool calls is not in the response, this thing will just be false. So that means we didn't either we didn't call web search and we didn't call our retriever tool. So then we're just going to fall back. Yep, right here. And this is just like a catch just in case the tool can make a decision. But most interestingly, here's where we choose a data source. Basically, so this is the output of our tool call. We're just going to fish out the name of the tool. So that's data source. And then here we go. If the day is horses web search, I'm returning web search as basically the next node to go to. Otherwise, if it's vector store return vector stores the next node to go to. So what's this web search thing will remember we write up here to find this node web search. That's it. We're just going to go to that node. What's this vector store? You'll see below how we can kind of tie these strings that we returned from the conditional edge to the node we want to go to. That's really it. Same kind of thing here. Decide to generate. That's going to roll in these two conditional edges into one. And basically it's going to do if there's no documents. So basically if we filtered out all of our documents from this first test here. Then what we're going to do is we've decided all documents are not relevant to the question. We're going to kick back to web search exactly as we show here. So that's this piece. Otherwise, we're going to go to generate. So that's this piece. So again, in these conditional edges, you're basically implementing the logic that each see in our diagram right here. That's all that's going on. And again, this is just implementing the final two checks for hallucinations and answer relevance. And yep, so here's our hallucination greater. We then extract the grade. If the if basically there are hallucinations. Oh, sorry. In this case, the grade actually, yes, means that the answer is grounded. So we say answer is actually grounded. And then we go to the next step. We go to the next test. That's all this is doing. So basically wrapping this logic that we're implementing here in our graph. So that's all it's going on. And let's go ahead and define all those things. So nice. We have all that. Now we can actually go down a little bit. And we can pull. This is actually where we'd stitched together everything. So all it's happening here is you see we defined all these functions up here. We just add them as nodes in our graph here. And then we build our graph here, basically by basically laying out the flow or the connectivity between our nodes and edges. So you know, you can look at this notebook to kind of study in a bit of detail what's going on. But frankly, what I like to do here typically just draw out a graph kind of we did up here. And then implement the logical flow here in your graph as nodes and edges, just like we're doing here. That's all that's happening. So again, we have like our entry point is the router. This is like the output is this is basically directing like here's what the router is outputting. And here's the next node to go to. So that's it. And then for each node, we're kind of deploying like we're saying like what's what's the flow. So web search goes to generate after. And retrieve goes to grade documents, grade documents kind of is is like is a conditional edge. Depending on the results, we either do web search or generate. And then our second one, we go from generate to basically this grade generation versus documents in question based on the output of that. We either have hallucinations, we regenerate. We found that the answer is not useful. We kick back to web search or we end. Finally, we have that LM fallback. And that's also if we go to the fallback, we end. So what you're seeing here is actually the logic flow. We're laying out in this graph matches the diagram that we laid out up top. I'm just going to copy is over it. I'll actually go then back to the diagram and kind of underscore that a little bit more. So here is the flow we've laid out. Again, here is our diagram. And you can kind of look at them side by side and see how they basically match up. So here's kind of our flow diagram going from basically query analysis. That's this thing, this route question. And you can see web search vector store LM fallback LM fallback web search vector store. So those are like the three options that can come out of this for conditional edge. And then here's where we connect. So if we go to web search, then basically we next go to generate. So that's kind of this whole flow. Now if we go to retrieve, then we're going to grade. So that's it. And you know, it follows kind of as you can see here, that's really it. So it's just nice to draw these diagrams out first. And then it's pretty quick to implement each node in each edge, just as a function. And then stitch them together in a graph, just like I show here. And of course, we'll make sure this code publish you can use as a reference. So there we go. Now let's try a few, a few different test questions. So like what player of the bears, I should draft and then I fell draft, right. Let's have a look at that and it should print everything is doing as we go. So, okay, this is important. Route question, it decides to route to web search. That's good. It doesn't go to our vector store. This is a current event, not related to our vector store at all. It goes to web search. And then it goes to generate. So that's what we'd expect. So basically web search goes through to generate. And we check hallucinations, generations ground the documents. We check generation versus question. The generation addresses the question. The Chicago Bears expected to draft Caleb Williams. That's right. That's that's the consensus. So cool. That works. Now let's ask a question related to our vector store. Where are the types of agent memory? We'll kick this off. So we're routing. Okay, we're out into rag. Now look how fast this is. That's really fast. So we basically whip through that document grading, determine the wrong relevant. We go to the season to generate. We check hallucinations. We check answer versus question. And there are several types of memory stored in the human brain memory can also store in general agents, you have LMP agents, memory stream retrieval model and flash them. It's representing what's captionable blog post pretty reasonably. Now let me show you something else is kind of nice. I can go to Lang Smith. And I can go to my projects. We create its new project co here adaptive rag at the start. And everything is actually logged there. Everything we just did. So I can open this up. And I can actually just kind of look through all the stages of my line graph to here's my retrieval stage. Here's my grade document stage and we can kind of audit the grading itself. We kind of look at this one by one previously, but it's actually pretty nice. We can actually audit every single individual document grade to see what's happening. We can basically go through to this is going to be one of the other graders here. Yep. So this is actually going to be hallucination grading right here. And then this is going to be the answer grading right here. So that's really it. You can kind of walk through the entire graph. You can you can kind of study what's going on, which is actually very useful. So it looks like this work pretty well. And finally, let's just ask a question that should go to that fall back path down at the bottom, like not related at all to our vector store current events. And yeah, hello, I'm doing well. So it's pretty neat. We've seen in maybe 15 minutes, we've from scratch built basically a semi sophisticated rack system that has agentic properties. We've done a line graph. We've done with co here command R. You can see it's pretty darn fast. In fact, we can go to Langsmith and look at so this whole thing took seven seconds. That is not bad. Let's look at the most recent one. So this takes one second. So the fallback mechanism to the LM is like one second. The let's just look here. So six seconds for the initial line graph. So this is not bad at all. It's quite fast. It does quite a few different checks. We do routing. And then we have kind of a bunch of nice fallback behavior and inline checking for both relevance hallucinations and answer kind of groundedness or answer usefulness. So you know, this is pretty nice. I definitely encourage you to play with a notebook command R is a really nice option for this due to the fact that it's tool use routing small and quite fast and it's really good for rags. It's a very nice kind of a very nice option for workflows like this. And I think you're going to see more and more of this kind of like adaptive or self reflective rag just because this is something that a lot of systems can benefit from like a lot of production rack systems kind of don't necessarily have fallbacks. Depending on, for example, like, you know, if the documents retrieved are not relevant. If the answer contains hallucinations and so forth. So this opportunity to play inline checking along with rag is like a really nice theme. I think we're going to see more and more of especially as model inference gets faster and faster and these checks get cheaper and cheaper to do kind of in the inference loop. Now as a final thing, I do want to bring up the point about, you know, we've shown this line graph stuff. What about agents, you know, how do you think about agents versus line graph, right? And I think the way I like to frame this is that line graph is really good for flows that you have kind of very clearly defined that don't have like kind of open endedness, but like in this case, we know what the steps we want to take every time we want to do basic query analysis routing and we want to do a three grading steps. And that's it. Line graph is really good for building very reliable flows. It's kind of putting an agent on guard rails and it's really nice. It's less flexible, but highly reliable. And so you can actually use smaller faster models with line graph. So that's the thing I like about we saw here, command are 35 billion parameter model works really well with line graph quite quick. We've able to implement pretty sophisticated rag flow really quickly, 15 minutes. Inference time is on the order of like less than you know, around five to six seconds. So so pretty good, right. Now what about agents, right. So I think agents come into play when you want more flexible workflows. You don't want to necessarily follow defined pattern. And you want an agent to be able to kind of reason and make open end decisions, which is interesting for certain like long horizon planning problems, you know, agents are really interesting. The catch is that over liabilities a bit worse with agents. And so, you know, that's a big question lot people bring up. And that's kind of where larger elements kind of come into play with aid, you know, there's been a lot of questions about using small LMS that even open source models with agents and reliability is kind of continuously being an issue. Whereas I've been able to run these types of land graphs with with like mistral or you know, command are actually open weights. You can run it locally. I've been able to run them very reproducibly with open source models on my laptop. So, you know, I think there's a trade off and commit actually there's a new co here model coming out, believe command our plus, which is a larger model. So it's probably more suitable for kind of more open ended, agente use cases. And there's actually a new integration with Langchain that support co here agents, which is quite nice. So I think it's it's worth experimenting for certain problems and workflows. You may need more open ended reasoning in which case using agent with a larger model. Otherwise, you can use like land graph for more more reliable potentially, but constrained flow and it can also use smaller models faster LMS. So there's some the tradeoffs to keep in mind, but anyway, encourage you play with a notebook, explore for yourself. I think command ours is a really nice model. I've also been experimenting with running it locally with a llama. Currently, the quantized model is like two bit quantized like 13 billion. So yeah, 13 gigs. It's a little bit too large to run quickly locally for me. Inference for things like rag were on the order of 30 seconds. So again, it's not great for a live demo, but it does work is available on a llama. So I encourage you to play with that. I have a mac M2 32 gig. So, you know, if I if you're a larger machine, then it absolutely could be worth working with locally. So encourage you play with that. Anyway, hopefully this was useful and interesting. I think this is a cool paper cool flow. Co here command ours a nice option for these types of like routing. It's quick, good with land graph, good for rag, good for tool use. So, you know, have a look and, you know, reply anything any feedback in the comments. Thanks. This is nice from Langshane. This is a talk I gave at two recent meetups in San Francisco called Israel really dead. And I figured since, you know, a lot of people actually weren't able to make those meetups. I just record this and put this on YouTube and see if this is of interest to folks. So we all kind of recognize that context windows are getting larger for LLMs. So on the X axis, you can see the tokens using pre training. That's of course, you know, getting larger as well. proprietary models are somewhere over the two trillion token regime. We don't quite know where they sit. And we all the way down to smaller malls like five to train on far fewer tokens. But what's really notable is on the Y axis, you can see about a year ago, they are models run order of 4,000, 8,000 tokens. And that's, you know, dozens of pages. We saw a cloud to come out with a 200,000 token model earlier, I think it was last year. GPD 4128,000 tokens. No, that's hundreds of pages. And now we're seeing cloud three in Gemini come out with million token models. So this is hundreds to thousands of pages. So because of this phenomenon, people have been kind of wondering, is ranked dead? If you can stuff, you know, many thousands of pages into the Congressman of an LLM, why do you need a renewable system? It's a good question. Spoke Spark a lot of mentioning debate on Twitter. And it's been first just kind of grounding on what is rag. So rag is really the process of reasoning and retrieval over chunks of information that they've been retrieved. It's starting with, you know, documents that are indexed. They're retrievable through some mechanism, typically some kind of cementing, some similarity search or keyword search, other mechanisms. Retrieved, dosher than past to an LLM. And the LLM reasons about them to ground response to the question in the retrieved documents. So that's kind of the overall flow. But the important point to make is that typically it's multiple documents and involve some form of reasoning. So one of the questions I asked recently is, you know, if longconics LLMs can replace rag, it should be able to perform, you know, multi-fact retrieval and reasoning from its own context really effectively. So I teamed up with Greg Cameron to kind of pressure test this. And he had done some really nice needle and the haystack analysis already focused on kind of single facts called needles placed in a haystack of Paul Gram essays. So I kind of extended that to kind of mirror the rag use case or kind of the rag context where I took multiple facts. So I call it multi-needle. I build on a funny needle in the haystack challenge published by Anthropic where they basically placed pizza ingredients in the context and asked the LLM to retrieve this combination of pizza ingredients. I did, I kind of rift on that and I basically split the pizza ingredients up into three different needles and placed those three ingredients in different places in the context and then asked the LLM to recover those three ingredients from the context. So again, the setup is the question is whether secret ingredients need to build up for pizza, the needles are the ingredients, figs, prosciutto, goat cheese. I'd place them in the context that some specified intervals to the way this test works is you can basically set the percent of context you want to place the first needle and the remaining two are placed at roughly equal intervals in the remaining context after the first. So that's kind of the way the test is set up now. It's all open source by the way the link is below. So the needles are placed. You ask a question, you prompt LLM with kind of with this context in the question and then produces the answer and now the framework will grade the response both one are all all the espacite the specified ingredients present in the answer and two if not which ones are missing. So I ran a bunch of analysis on this with GPT-4 and came kind of came up with some some fun results. So you can see on the left here what this is looking at is different numbers of needles placed in 120,000 token context window for GPT-4 and I'm asking GPT-4 to retrieve either one, three or ten needles. Now I'm also asking it to do reasoning on those needles that's what you can see in those red bars. So green is just retrieved the ingredients red is reasoning and the reasoning challenge here is just return the first letter of each ingredient. So we find is basically two things the performance or the percentage of needles retrieved drops with respect to the number of needles does kind of intuitive you place more facts performance gets worse. But also it gets worse if you ask it to reason. So if you say just return the needles it does a little bit better than if you say return the needles and tell me the first letter so you overlay reasoning. So this is the first observation more facts is harder and reasoning is harder than just retrieval. Now the second question we ask is where are these needles actually present in the context that we're missing right so we know for example retrieval of ten needles is around 60%. So where are the missing needles in the context so on the right you can see results telling us actually which specific needles are the model fails to retrieve. So we can see is as you go from a thousand tokens up to 120 thousand tokens on the X here and you look at needle one place at the star of the document to needle ten placed at the end at a thousand token context length you can retrieve them all. So again kind of actually see over here small actually sorry over here everything I'm looking at 120 thousand tokens so that's really not the point. The point is actually smaller context better retrieval so that's kind of point one as I increase the context window I actually see that there is increased failure to retrieve needles which you see can see in red here towards the start of the document. So this is an interesting result and it actually matches what Greg saw with single needle case as well so the way to think about it it appears that you know if you for example read a book and I ask you a question about the first chapter you might have forgotten it same kind of phenomenon appears to happen here with retrieval where needles towards the start of the context are kind of forgotten or are not well retrieved relative to those of the end. So this is the fact we see with GPD for it's been reproduced quite a bit so I ran nine different trials here Greg's also seen this repeatedly with single needle so it seems like a pretty consistent result. And there's an interesting point I put this on Twitter and a number of folks you know replied and someone sent me this paper which is pretty interesting and it mentions recently bias is one possible reason so the most informative tokens for predicting the next token. You know are are present close to or recent to kind of where you're doing your generation and so there's a bias to attend to recent tokens which is obviously not great for the retrieval problem as we saw here so again the results show us that. Retrieving is a bit harder than retrieval more needles is more difficult and needles towards the start of your context are harder retrieved than towards the end those are three main observations from this. And it may be indeed do this recency bias so overall what this kind of tells you is be wary of this context stuffing in large long context elements there are no retrieval guarantees and also there's some recent results that came out action just today suggesting that single needle may be misleadingly easy. You know there's no reasoning it's retrieving a single needle and also these guys I showed this tweet here so that the in a lot of these needle and haystack challenges including mine the facts that we look for are very different than the background kind of haystack of Paul Grammys says and so that may be kind of an interesting artifact they note that indeed if the needle is more subtle retrieval is worse so. I think basically when you see these really strong performing needle and the haystack analysis put up by model providers you should be skeptical you shouldn't necessarily assume that you're going to get high quality retrieval from these long context elements for numerous reasons you need to think about retrieval of multiple facts you need to think about reasoning on top of retrieval you need to think about the subtlety of the retrieval relative to the background context because for many of these needle the haystack challenges it's a single needle no matter what you're looking for. There's no single needle no reasoning and the needle itself is very different from the background so anyway those may all make the challenge a bit easier than a real world scenario of fact retrieval so I just want to lay out that cautionary notes but you know I think it is fair to say this will certainly get better and I think it's also fair to say that rag will change and this is just like a nearly not a great joke but Frank Zappa musician made the point jazz is dead it just smells funny you know I think same for rag rag is not dead but it will change I think that's like kind of the key point here so just as a follow up on that rag today's focus on precise retrieval of relevant doc chunks so it's very focused on typically taking documents chunking them in some particular way often using very idiosyncratic chunking methods things like chunk size are kind of picked almost arbitrarily embedding them storing them in index taking a question embedding it doing K&N similarity search to retrieve relevant chunks you're often setting a K parameter which is the number of chunks you retrieve you often will do some kind of filtering or processing on the retrieve chunks and then ground your answer in those retrieved chunks so it's very focused on precise retrieval of just the right chunks now in a world where you have very long context models I think there's an int the a fair question to ask is is this really kind of the most reasonable approach so kind of on the left here you can kind of see this notion closer to today of I need the exact relevant chunk you can risk of engineering you can have you know higher complexity sensitivity to these odd parameters like chunk size K and you can need suffer lower recall because you're really only picking very precise chunks you're beholden to our particular embedding models so you know I think going forward as long context models get better and better there are definitely questions you should certainly question the current kind of very precise chunking rag paradigm but on the flip side I think just throwing all your docs in the context probably will also not be a good example of the context probably will also not be the preferred approach you'll suffer higher latency higher token usage I should note that today 100,000 token GPT-4 is like $1 per generation I spent a lot of money online chains account on that multinatal analysis I don't want to tell Harrison how much I spent so it's you know it's not good right you can't on retrieval and security not an authentication are issues if for example you need different users different different access to certain kind of retreat documents or chunks in the context stuff in case you kind of can't manage security as easily so there's probably some predo optimal regime kind of here in the middle and you know I put this out on Twitter I think there's some reasonable points raise I think you know this inclusion at the document levels probably pretty sane documents are self contained chunks of context so you know what about document centric rag so no chunking but just like operate on the context of full documents so you know if you think forward to the rag paradigm as document centric you still have the problem of taking an input question routing it to the right document this doesn't change so I think a lot of methods that we think about for kind of query analysis taking an input question rewriting it in a certain way to optimize retrieval things like routing taking a question routing it to the right database be it a relational database graph database vector store and query construction methods so for example text-to-sequel text-to-cyper for graphs or text to even like metadata filters for for vector stores those are all still relevant in the world that you have long context lm's you're probably not going to dump your entire SQL DB and feed that to the lm you're still going to have SQL queries you're still going to have graph queries you may be more permissive with what you extract but it still is very reasonable to store the majority of your structure data in these in these forms likewise with unstructured data like documents like we said before it still probably makes sense to to you know store documents independently but just simply treat full documents rather than worrying about these idiosyncratic parameters like chunk size and along those lines there's a lot of methods out there we've we've done a few of these that are kind of well optimized for document retrieval so one of one of flag is what we call multi representation indexing and there's actually a really nice paper on this called dense extracreaver or proposition indexing but the main point is simply this what you do is you take your document you produce a representation like a summary of that document you index that some summary right and then at retrieval time you ask your question you embed your question and you simply use a high level summary to just retrieve the right document you pass the full document to the lm for a kind of final generation so it's kind of a nice trick where you don't have to worry about embedding full documents in this particular case you can use kind of very nice descriptive summarization prompts to build descriptive summaries and the problem you're here is just get me the right document it's an easier problem than get me the right chunk so this is kind of a nice approach it there's also different variants up it was I share below when it's called parent document retriever where you could use in principle if you wanted smaller chunks but then just return full documents but anyway the point is preserving full documents for generation but using representations like some research chunks for retrieval so that's kind of like approach one that I think is really interesting approach to is this idea of raptor is a cool paper came out of stamp or summer recently and this solves a problem of what if certain questions I need to integrate information across many documents so what this approach does is it takes documents and it embeds them and clusters them and then it summarizes each cluster and it does this recursively until you land up with only one very high level summary for the entire corpus of documents and what they do is they take this kind of this abstraction hierarchy so to speak of different documents summarizations and they just index all of it and they use this in retrieval and so basically if you have a question that draws an information across numerous documents you probably have a summary present and index that kind of has that answer captured so it's a nice trick to consolidate information across documents there are paper actually reports you know these documents in their case or the Leafs are actually document chunks or slices but I actually showed I've a video on it in a notebook that this works across full documents as well and this and I said way into to do this you do need to think about long context and vetting models because you're embedding full documents and that's a really interesting thing to track the you know hazy research put out a really nice blog post on this using with the Monarch Mixers it's kind of a new architecture that attends to longer context they have a 32,000 token embedding model that's a bill one together AI absolutely worth experimenting with I think is really interesting trend so long comics and beddings kind of play really well with this kind of idea we take full documents embed them using for example long context and vetting models and you can kind of build these documents summarization trees really effectively so I think it's another nice trick for working with full documents in the long context kind of LLM regime one other thing on note I think there's also going to move move the move away from kind of single shot rag was today's rag we typically you know we chunk documents and embed them store them in index you know do retrieval and then do generation but there's no reason why you shouldn't kind of do reasoning on top of the generation or raising on top of the retrieval and feedback if there are errors so there's a really nice paper called self rag that kind of reports this we implement this using the land graph works really well and the simply did simply to you know grade the relevance of your documents relative to your question first if they're not relevant you rewrite the question you can you can do many things in this case we do question rewriting and try again we also grade for hallucinations we grade for answer relevance but anyway it kind of moves rag from a kind of single shot paradigm to a kind of a cyclic flow in which you actually do various gradings downstream and this is all relevant in the long context LLM regime as well in fact you know it you absolutely should take advantage of of for example increasingly fast and perform an LLM to do these grading framework site land graph allow you to build these kind of these flows which build which allows you to kind of have a more performant kind of kind of self reflective rag pipeline now I dig in a lot of questions about latency here and I completely agree there's a trade up between kind of performance accuracy and latency that's present here I think the real answer is you can opt to use very fast for example models like Grock we're seeing you know GPT35 turbos very fast these are fairly easy grading challenges so you can use very very fast LLM to do the grading and for example you you can also restrict this to only do one turn of a kind of cyclic iteration so you can kind of restrict the latency in that way as well so anyway I think it's a really cool approach still relevant in the world as we move towards longer context so it's kind of like building reasoning on top of rag in the generation and retrieval stages and a related point one of the challenges with rag is that your index for example you may have a question that is that asks something that's outside the scope of your index and this is kind of always a problem so a really cool paper called C rag or corrective rag came out you know a couple months ago that basically does grading just like we talked about before and then if the documents are not relevant you kick off into a web search and basically return the search results to the LLM for final generation so it's a nice fallback in cases where um you're you know the questions out of the domain of your retriever so you know again nice trick overlaying reasoning on top of a rag I think this trend you know continues um because you know it just it makes rag systems you know more performant uh and less brittle to questions that are out of domain so you know that's another kind of nice idea this particular approach also we showed works really well with with with open source model so I ran this with mistral 7b it can run locally on my laptop he's an alama so again really nice approach I encourage you to look into this um and this is all kind of independent of the LLM kind of context length these are reasoning you can add on top of the retrieval stage that that can kind of improve overall performance and so the overall picture kind of looks like this where you know I think that the the problem of routing your question to the right database and or to the right document kind of remains in place queer analysis is still quite relevant routing is still relevant queer construction is still relevant um in the long context regime I think there is less of an emphasis on document chunking working with full documents is probably kind of more preto optimal so to speak um there's some some clever tricks for end is indexing of documents like the multi representation indexing we talked about the hierarchical indexing using raptor that we talked about as well are two interesting ideas for document centric indexing um and then kind of recent in generation post retrieval on retrieval itself to grade on the generations themselves checking for hallucinations those are all kind of interesting and relevant parts of of rag system that I think we'll probably will see more and more as you move more away from like a more naive prompt response paradigm more to like a flow paradigm we're seeing that actually our d into generation is probably going to carry over to rag as well where we kind of build rags systems that have kind of a cyclic flow to them operate on documents use long comics L lens um it's still used kind of routing your analysis so reasoning before pre retrieval reasoning post retrieval so anyway that was kind of my talk um and yeah feel free to leave any comments on the video and I'll try to answer any questions but um yeah that's that's probably about it thank you\n",
      "\n",
      " Let's talk about RAG versus fine tuning. Now, they're both powerful ways to enhance the capabilities of large language models. But today, you're going to learn about their strengths, their use cases, and how you can choose between them. So one of the biggest issues with dealing with genitive AI right now is one, enhancing the models, but also two, dealing with their limitations. For example, I just recently asked my favorite LLM a simple question, who won the Euro 2024 World Championship. And while this might seem like a simple query for my model, well, there's a slight issue because the model wasn't trained on that specific information. It can't give me an accurate or up-to-date answer. At the same time, these popular models are very generalistic. And so how do we think about specializing them for specific use cases and adapt them in enterprise applications? Because your data is one of the most important things that you can work with. And in the field of AI, using techniques such as RAG or fine tuning will allow you to supercharge the capabilities that your application delivers. So in the next few minutes, we're going to learn about both of these techniques, the differences between them, and where you can start seeing and using them in. Let's get started. So let's begin with retrieval augmented generation, which is a way to increase the capabilities of a model through retrieving external and up-to-date information, augmenting the original prompt that was given to the model, and then generating a response back using that context and information. And this is really powerful because if we think back about that example of what the Euro Cup, well, the model didn't have the information in context to provide an answer. And this is one of the big limitations of LLMs, but this is mitigated in a way with RAG. Because now, instead of having an incorrect or possibly hallucinated answer, we're able to work with what's known as a corpus of information. So this could be data, this could be PDFs, documents, spreadsheets, things that are relevant to our specific organization or knowledge that we need to specialize in. When the query comes in this time, we're working with what's known as a retriever that's able to pull the correct documents and relative context to what the question is, and then pass that knowledge, as well as the original prompt to a large language model. And with its intuition and pre-trained data, it's able to give us a response back based on that contextualized information, which is really, really powerful. Because we can start to see that we can get better responses back from a model with our proprietary and confidential information without needing to do any retraining on the model. And this is a great and popular way to enhance the capabilities of a model without having to do any fine tuning. So as the name implies, what this involves is taking a large language foundational model. But this time, we're going to be specializing it in a certain domain or area. So we're working with labeled and targeted data that's going to be provided to the model. And when we do some processing, we'll have a specialized model for a specific use case to talk in a certain style, to have a certain tone that could represent our organization or company. And so then when a model is queried from a user or any other type of way, we'll have a response that gives the correct tone and output or specialty in a domain that we'd like to receive. And this is really important because what we're doing is essentially baking in this context and intuition into the model. And it's really important because this is now a part of the model's weights versus being supplemented on top with a technique like RAG. OK, so we understand how both of these techniques can enhance a model's accuracy, output, and performance. But let's take a look at their strengths and weaknesses in some common use cases. Because of the direction that you go in can greatly affect a model's performance, its accuracy, outputs, compute costs, and much, much more. So let's begin with retrieval augmented generation. And something that I want to point out here is that because we're working with a corpus of information and data, this is perfect for dynamic data sources, such as databases and other data repositories, where we want to continuously pull information and have that up to date for the model to use and understand. And at the same time, because we're working with this retriever system and passing in the information as context in the prompt, well, that really helps with hallucinations. And providing the sources for this information is really important in systems where we need trust and transparency when we're using AI. So this is fantastic. But let's also think about this whole system because having this efficient retrieval system is really important in how we select and pick the data that we want to provide in that limited context window. And so maintaining this is also something that you need to think about. And at the same time, what we're doing here in this system is effectively supplementing that information on top of the model. So we're not essentially enhancing the base model itself. We're just giving it the relative and contextual information it needs versus fine tuning is a little bit different because we're actually baking in that context and intuition into the model. Well, we have greater influence in essentially how the model behaves and reacts in different situations. Is it an insurance adjuster? Can it summarize documents? Whatever we want the model to do, we can essentially use fine tuning in order to help with that process. And at the same time, because that is baked into the model's weights itself, well, that's really great for speed and inference cost and a variety of other factors that come to running models. So for example, we can use smaller prompt context windows in order to get the responses that we want from the model. And as we begin to specialize these models, they can get smaller and smaller for a specific use case. So it's really great for running these specific specialized models in a variety of use cases. But at the same time, we have the same issue of cutoff. So up until the point where the model is trained, well, after that, we have no more additional information that we can give to the model. So the same issue that we had with the World Cup example. So both of these have their strengths and weaknesses, but let's actually see this in some examples and use cases here. So when you're thinking about choosing between rag and fine tuning, it's really important to consider your AI enabled applications, priorities, and requirements. So namely, this starts off with the data. Is the data that you're working with slow moving or is it fast? For example, if we need to use up-to-date external information and have that ready contextually every time we use a model, then this could be a great use case for rag. For example, a product documentation chatbot where we can continually update the responses with up-to-date information. Now at the same time, let's think about the industry that you might be in. Now fine tuning is really powerful for specific industries that have nuances in their writing styles, terminology, vocabulary. And so for example, if we have a legal document summarizer, well, this could be a perfect use case for fine tuning. Now let's think about sources. This is really important right now in having transparency behind our models. And with rag being able to provide the context and where the information came from, is really, really great. And so this could be a great use case again for that chatbot, for retail, insurance, and a variety of other specialities where having that source and information in the context of the prompt is very important. But at the same time, we may have things such as past data in our organization that we can use to train a model. So let it be accustomed to the data that we're going to be working with. For example, again, that legal summarizer could have past data on different legal cases and documents that we feed it so that it understands the situation that it's working in, and we have better, more desirable outputs. So this is cool, but I think the best situation is a combination of both of these methods. So let's say we have a financial news reporting service. Well, we could fine tune it to be native to the industry of finance and understand all the lingo there. We could also give it past data of financial records and let it understand how we work in that specific industry, but also be able to provide the most up-to-date sources for news and data, and be able to provide that with a level of confidence and transparency and trust to the end user who is making that decision and needs to know the source. And this is really where a combination of fine tuning and rag is so awesome, because we can really build amazing applications, taking advantage of both rag as a way to retrieve that information and have it up-to-date, but fine tuning to specialize our data, but also specialize our model in a certain domain. So they're both wonderful techniques, and they have their strengths, but the choice to use one or a combination of both techniques is up to you in your specific use case and data. So thank you so much for watching. As always, if you have any questions about fine tuning, rag, or all AI related topics, let us know in the comment section below. Don't forget to like the video and subscribe to the channel for more content. Thanks so much for watching.\n",
      "Human: Faça um resumo desse video\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' O vídeo discute sobre como as pessoas podem adotar uma abordagem mais positiva e saudável para o uso de tecnologia. Ele enfatiza a importância da consciência digital, onde as pessoas se tornam mais conscientes do impacto que o uso da tecnologia tem em sua vida, incluindo a maneira como ela pode afetar a saúde mental e emocional.\\n\\nO vídeo também destaca alguns dos riscos associados ao uso excessivo de tecnologia, como ansiedade, depressão e baixo desempenho acadêmico. Além disso, ele oferece algumas dicas práticas para adotar uma abordagem mais sintonizada com a vida real, incluindo definir limites para o uso da tecnologia, usando técnicas de organização para evitar distração e tomando pequenos pauses ao longo do dia para se desconectar e relaxar.\\n\\nO vídeo também apresenta alguns benefícios potenciais da tecnologia, incluindo a capacidade de conectar-se com pessoas em todo o mundo e ter acesso a uma grande quantidade de informação. Por fim, ele incentiva as pessoas a encontrarem um equilíbrio saudável entre a vida real e a tecnologia para se tornarem mais produtivos e felizes em seu uso da tecnologia.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pergunta = \"Faça um resumo desse video\"\n",
    "chain.run(input_documents = docs, question = pergunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3738e36",
   "metadata": {},
   "source": [
    "## Web via URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac50baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.web_base import WebBaseLoader\n",
    "\n",
    "url = \"https://python.langchain.com/docs/tutorials/rag/\"\n",
    "loadd = WebBaseLoader(url)\n",
    "\n",
    "docu = loadd.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131e24da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c7fe0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nBuild a Retrieval Augmented Generation (RAG) App: Part 1 | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuil'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docu[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e229afa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Build a Retrieval Augmented Generation (RAG) App: Part 1 | 🦜️🔗 LangChain\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a Retrieval Augmented Generation (RAG) App: Part 1On this pageBuild a Retrieval Augmented Generation (RAG) App: Part 1\n",
      "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
      "This is a multi-part tutorial:\n",
      "\n",
      "Part 1 (this guide) introduces RAG and walks through a minimal implementation.\n",
      "Part 2 extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\n",
      "\n",
      "This tutorial will show how to build a simple Q&A application\n",
      "over a text data source. Along the way we’ll go over a typical Q&A\n",
      "architecture and highlight additional resources for more advanced Q&A techniques. We’ll also see\n",
      "how LangSmith can help us trace and understand our application.\n",
      "LangSmith will become increasingly helpful as our application grows in\n",
      "complexity.\n",
      "If you're already familiar with basic retrieval, you might also be interested in\n",
      "this high-level overview of different retrieval techniques.\n",
      "Note: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing question/answering over SQL data.\n",
      "Overview​\n",
      "A typical RAG application has two main components:\n",
      "Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
      "Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
      "Note: the indexing portion of this tutorial will largely follow the semantic search tutorial.\n",
      "The most common full sequence from raw data to answer looks like:\n",
      "Indexing​\n",
      "\n",
      "Load: First we need to load our data. This is done with Document Loaders.\n",
      "Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
      "Store: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n",
      "\n",
      "\n",
      "Retrieval and generation​\n",
      "\n",
      "Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
      "Generate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\n",
      "\n",
      "\n",
      "Once we've indexed our data, we will use LangGraph as our orchestration framework to implement the retrieval and generation steps.\n",
      "Setup​\n",
      "Jupyter Notebook​\n",
      "This and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\n",
      "Installation​\n",
      "This tutorial requires these langchain dependencies:\n",
      "\n",
      "PipConda%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraphconda install langchain-text-splitters langchain-community langgraph -c conda-forge\n",
      "For more details, see our Installation guide.\n",
      "LangSmith​\n",
      "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\n",
      "As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\n",
      "The best way to do this is with LangSmith.\n",
      "After you sign up at the link above, make sure to set your environment variables to start logging traces:\n",
      "export LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\n",
      "Or, if in a notebook, you can set them with:\n",
      "import getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
      "Components​\n",
      "We will need to select three components from LangChain's suite of integrations.\n",
      "\n",
      "Select chat model:Google Gemini▾OpenAIAnthropicAzureGoogle GeminiGoogle VertexAWSGroqCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabricksxAIPerplexitypip install -qU \"langchain[google-genai]\"import getpassimport osif not os.environ.get(\"GOOGLE_API_KEY\"):  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")from langchain.chat_models import init_chat_modelllm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
      "\n",
      "Select embeddings model:OpenAI▾OpenAIAzureGoogle GeminiGoogle VertexAWSHuggingFaceOllamaCohereMistralAINomicNVIDIAVoyage AIIBM watsonxFakepip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
      "\n",
      "Select vector store:In-memory▾In-memoryAstraDBChromaFAISSMilvusMongoDBPGVectorPineconeQdrantpip install -qU langchain-corefrom langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)\n",
      "Preview​\n",
      "In this guide we’ll build an app that answers questions about the website's content. The specific website we will use is the LLM Powered Autonomous\n",
      "Agents blog post\n",
      "by Lilian Weng, which allows us to ask questions about the contents of\n",
      "the post.\n",
      "We can create a simple indexing pipeline and RAG chain to do this in ~50\n",
      "lines of code.\n",
      "import bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Index chunks_ = vector_store.add_documents(documents=all_splits)# Define prompt for question-answering# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    context: List[Document]    answer: str# Define application stepsdef retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}# Compile application and testgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:hub | WebBaseLoader | Document | RecursiveCharacterTextSplitter | StateGraph\n",
      "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(response[\"answer\"])\n",
      "Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate easier execution and understanding. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think step-by-step, allowing them to explore multiple reasoning possibilities. This method enhances performance on complex tasks and provides insight into the model's thinking process.\n",
      "Check out the LangSmith\n",
      "trace.\n",
      "Detailed walkthrough​\n",
      "Let’s go through the above code step-by-step to really understand what’s\n",
      "going on.\n",
      "1. Indexing​\n",
      "noteThis section is an abbreviated version of the content in the semantic search tutorial.\n",
      "If you're comfortable with document loaders, embeddings, and vector stores,\n",
      "feel free to skip to the next section on retrieval and generation.\n",
      "Loading documents​\n",
      "We need to first load the blog post contents. We can use\n",
      "DocumentLoaders\n",
      "for this, which are objects that load in data from a source and return a\n",
      "list of\n",
      "Document\n",
      "objects.\n",
      "In this case we’ll use the\n",
      "WebBaseLoader,\n",
      "which uses urllib to load HTML from web URLs and BeautifulSoup to\n",
      "parse it to text. We can customize the HTML -> text parsing by passing\n",
      "in parameters into the BeautifulSoup parser via bs_kwargs (see\n",
      "BeautifulSoup\n",
      "docs).\n",
      "In this case only HTML tags with class “post-content”, “post-title”, or\n",
      "“post-header” are relevant, so we’ll remove all others.\n",
      "import bs4from langchain_community.document_loaders import WebBaseLoader# Only keep post title, headers, and content from the full HTML.bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs={\"parse_only\": bs4_strainer},)docs = loader.load()assert len(docs) == 1print(f\"Total characters: {len(docs[0].page_content)}\")API Reference:WebBaseLoader\n",
      "Total characters: 43131\n",
      "print(docs[0].page_content[:500])\n",
      "      LLM Powered Autonomous Agents    Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian WengBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In\n",
      "Go deeper​\n",
      "DocumentLoader: Object that loads data from a source as list of Documents.\n",
      "\n",
      "Docs:\n",
      "Detailed documentation on how to use DocumentLoaders.\n",
      "Integrations: 160+\n",
      "integrations to choose from.\n",
      "Interface:\n",
      "API reference for the base interface.\n",
      "\n",
      "Splitting documents​\n",
      "Our loaded document is over 42k characters which is too long to fit\n",
      "into the context window of many models. Even for those models that could\n",
      "fit the full post in their context window, models can struggle to find\n",
      "information in very long inputs.\n",
      "To handle this we’ll split the Document into chunks for embedding and\n",
      "vector storage. This should help us retrieve only the most relevant parts\n",
      "of the blog post at run time.\n",
      "As in the semantic search tutorial, we use a\n",
      "RecursiveCharacterTextSplitter,\n",
      "which will recursively split the document using common separators like\n",
      "new lines until each chunk is the appropriate size. This is the\n",
      "recommended text splitter for generic text use cases.\n",
      "from langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000,  # chunk size (characters)    chunk_overlap=200,  # chunk overlap (characters)    add_start_index=True,  # track index in original document)all_splits = text_splitter.split_documents(docs)print(f\"Split blog post into {len(all_splits)} sub-documents.\")API Reference:RecursiveCharacterTextSplitter\n",
      "Split blog post into 66 sub-documents.\n",
      "Go deeper​\n",
      "TextSplitter: Object that splits a list of Documents into smaller\n",
      "chunks. Subclass of DocumentTransformers.\n",
      "\n",
      "Learn more about splitting text using different methods by reading the how-to docs\n",
      "Code (py or js)\n",
      "Scientific papers\n",
      "Interface: API reference for the base interface.\n",
      "\n",
      "DocumentTransformer: Object that performs a transformation on a list\n",
      "of Document objects.\n",
      "\n",
      "Docs: Detailed documentation on how to use DocumentTransformers\n",
      "Integrations\n",
      "Interface: API reference for the base interface.\n",
      "\n",
      "Storing documents​\n",
      "Now we need to index our 66 text chunks so that we can search over them\n",
      "at runtime. Following the semantic search tutorial,\n",
      "our approach is to embed the contents of each document split and insert these embeddings\n",
      "into a vector store. Given an input query, we can then use\n",
      "vector search to retrieve relevant documents.\n",
      "We can embed and store all of our document splits in a single command\n",
      "using the vector store and embeddings model selected at the start of the tutorial.\n",
      "document_ids = vector_store.add_documents(documents=all_splits)print(document_ids[:3])\n",
      "['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']\n",
      "Go deeper​\n",
      "Embeddings: Wrapper around a text embedding model, used for converting\n",
      "text to embeddings.\n",
      "\n",
      "Docs: Detailed documentation on how to use embeddings.\n",
      "Integrations: 30+ integrations to choose from.\n",
      "Interface: API reference for the base interface.\n",
      "\n",
      "VectorStore: Wrapper around a vector database, used for storing and\n",
      "querying embeddings.\n",
      "\n",
      "Docs: Detailed documentation on how to use vector stores.\n",
      "Integrations: 40+ integrations to choose from.\n",
      "Interface: API reference for the base interface.\n",
      "\n",
      "This completes the Indexing portion of the pipeline. At this point\n",
      "we have a query-able vector store containing the chunked contents of our\n",
      "blog post. Given a user question, we should ideally be able to return\n",
      "the snippets of the blog post that answer the question.\n",
      "2. Retrieval and Generation​\n",
      "Now let’s write the actual application logic. We want to create a simple\n",
      "application that takes a user question, searches for documents relevant\n",
      "to that question, passes the retrieved documents and initial question to\n",
      "a model, and returns an answer.\n",
      "For generation, we will use the chat model selected at the start of the tutorial.\n",
      "We’ll use a prompt for RAG that is checked into the LangChain prompt hub\n",
      "(here).\n",
      "from langchain import hub# N.B. for non-US LangSmith endpoints, you may need to specify# api_url=\"https://api.smith.langchain.com\" in hub.pull.prompt = hub.pull(\"rlm/rag-prompt\")example_messages = prompt.invoke(    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}).to_messages()assert len(example_messages) == 1print(example_messages[0].content)API Reference:hub\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.Question: (question goes here) Context: (context goes here) Answer:\n",
      "We'll use LangGraph to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:\n",
      "\n",
      "We can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.\n",
      "We get streamlined deployments via LangGraph Platform.\n",
      "LangSmith will automatically trace the steps of our application together.\n",
      "We can easily add key features to our application, including persistence and human-in-the-loop approval, with minimal code changes.\n",
      "\n",
      "To use LangGraph, we need to define three things:\n",
      "\n",
      "The state of our application;\n",
      "The nodes of our application (i.e., application steps);\n",
      "The \"control flow\" of our application (e.g., the ordering of the steps).\n",
      "\n",
      "State:​\n",
      "The state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel.\n",
      "For a simple RAG application, we can just keep track of the input question, retrieved context, and generated answer:\n",
      "from langchain_core.documents import Documentfrom typing_extensions import List, TypedDictclass State(TypedDict):    question: str    context: List[Document]    answer: strAPI Reference:Document\n",
      "Nodes (application steps)​\n",
      "Let's start with a simple sequence of two steps: retrieval and generation.\n",
      "def retrieve(state: State):    retrieved_docs = vector_store.similarity_search(state[\"question\"])    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}\n",
      "Our retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model.\n",
      "Control flow​\n",
      "Finally, we compile our application into a single graph object. In this case, we are just connecting the retrieval and generation steps into a single sequence.\n",
      "from langgraph.graph import START, StateGraphgraph_builder = StateGraph(State).add_sequence([retrieve, generate])graph_builder.add_edge(START, \"retrieve\")graph = graph_builder.compile()API Reference:StateGraph\n",
      "LangGraph also comes with built-in utilities for visualizing the control flow of your application:\n",
      "from IPython.display import Image, displaydisplay(Image(graph.get_graph().draw_mermaid_png()))\n",
      "\n",
      "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:question = \"...\"retrieved_docs = vector_store.similarity_search(question)docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)prompt = prompt.invoke({\"question\": question, \"context\": docs_content})answer = llm.invoke(prompt)The benefits of LangGraph include:\n",
      "Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n",
      "Automatic support for tracing via LangSmith and deployments via LangGraph Platform;\n",
      "Support for persistence, human-in-the-loop, and other features.\n",
      "Many use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in Part 2 of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n",
      "Usage​\n",
      "Let's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n",
      "Invoke:\n",
      "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})print(f'Context: {result[\"context\"]}\\n\\n')print(f'Answer: {result[\"answer\"]}')\n",
      "Context: [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]Answer: Task decomposition is a technique used to break down complex tasks into smaller, manageable steps, allowing for more efficient problem-solving. This can be achieved through methods like chain of thought prompting or the tree of thoughts approach, which explores multiple reasoning possibilities at each step. It can be initiated through simple prompts, task-specific instructions, or human inputs.\n",
      "Stream steps:\n",
      "for step in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"):    print(f\"{step}\\n\\n----------------\\n\")\n",
      "{'retrieve': {'context': [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]}}----------------{'generate': {'answer': 'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This technique, often enhanced by methods like Chain of Thought (CoT) or Tree of Thoughts, allows models to reason through tasks systematically and improves performance by clarifying the thought process. It can be achieved through simple prompts, task-specific instructions, or human inputs.'}}----------------\n",
      "Stream tokens:\n",
      "for message, metadata in graph.stream(    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"messages\"):    print(message.content, end=\"|\")\n",
      "|Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| can| be| achieved| through| techniques| like| Chain| of| Thought| (|Co|T|)| prompting|,| which| encourages| the| model| to| think| step| by| step|,| or| through| more| structured| methods| like| the| Tree| of| Thoughts|.| This| approach| not| only| simplifies| task| execution| but| also| provides| insights| into| the| model|'s| reasoning| process|.||\n",
      "tipFor async invocations, use:result = await graph.ainvoke(...)andasync for step in graph.astream(...):\n",
      "Returning sources​\n",
      "Note that by storing the retrieved context in the state of the graph, we recover sources for the model's generated answer in the \"context\" field of the state. See this guide on returning sources for more detail.\n",
      "Go deeper​\n",
      "Chat models take in a sequence of messages and return a message.\n",
      "\n",
      "Docs\n",
      "Integrations: 25+ integrations to choose from.\n",
      "Interface: API reference for the base interface.\n",
      "\n",
      "Customizing the prompt\n",
      "As shown above, we can load prompts (e.g., this RAG\n",
      "prompt) from the prompt\n",
      "hub. The prompt can also be easily customized. For example:\n",
      "from langchain_core.prompts import PromptTemplatetemplate = \"\"\"Use the following pieces of context to answer the question at the end.If you don't know the answer, just say that you don't know, don't try to make up an answer.Use three sentences maximum and keep the answer as concise as possible.Always say \"thanks for asking!\" at the end of the answer.{context}Question: {question}Helpful Answer:\"\"\"custom_rag_prompt = PromptTemplate.from_template(template)API Reference:PromptTemplate\n",
      "Query analysis​\n",
      "So far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:\n",
      "\n",
      "In addition to semantic search, we can build in structured filters (e.g., \"Find documents since the year 2020.\");\n",
      "The model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries.\n",
      "\n",
      "Query analysis employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let's add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later.\n",
      "total_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"all_splits[0].metadata\n",
      "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8, 'section': 'beginning'}\n",
      "We will need to update the documents in our vector store. We will use a simple InMemoryVectorStore for this, as we will use some of its specific features (i.e., metadata filtering). Refer to the vector store integration documentation for relevant features of your chosen vector store.\n",
      "from langchain_core.vectorstores import InMemoryVectorStorevector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)API Reference:InMemoryVectorStore\n",
      "Let's next define a schema for our search query. We will use structured output for this purpose. Here we define a query as containing a string query and a document section (either \"beginning\", \"middle\", or \"end\"), but this can be defined however you like.\n",
      "from typing import Literalfrom typing_extensions import Annotatedclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]\n",
      "Finally, we add a step to our LangGraph application to generate a query from the user's raw input:\n",
      "class State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()\n",
      "Full Code:from typing import Literalimport bs4from langchain import hubfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.documents import Documentfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langgraph.graph import START, StateGraphfrom typing_extensions import Annotated, List, TypedDict# Load and chunk contents of the blogloader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)all_splits = text_splitter.split_documents(docs)# Update metadata (illustration purposes)total_documents = len(all_splits)third = total_documents // 3for i, document in enumerate(all_splits):    if i < third:        document.metadata[\"section\"] = \"beginning\"    elif i < 2 * third:        document.metadata[\"section\"] = \"middle\"    else:        document.metadata[\"section\"] = \"end\"# Index chunksvector_store = InMemoryVectorStore(embeddings)_ = vector_store.add_documents(all_splits)# Define schema for searchclass Search(TypedDict):    \"\"\"Search query.\"\"\"    query: Annotated[str, ..., \"Search query to run.\"]    section: Annotated[        Literal[\"beginning\", \"middle\", \"end\"],        ...,        \"Section to query.\",    ]# Define prompt for question-answeringprompt = hub.pull(\"rlm/rag-prompt\")# Define state for applicationclass State(TypedDict):    question: str    query: Search    context: List[Document]    answer: strdef analyze_query(state: State):    structured_llm = llm.with_structured_output(Search)    query = structured_llm.invoke(state[\"question\"])    return {\"query\": query}def retrieve(state: State):    query = state[\"query\"]    retrieved_docs = vector_store.similarity_search(        query[\"query\"],        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],    )    return {\"context\": retrieved_docs}def generate(state: State):    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})    response = llm.invoke(messages)    return {\"answer\": response.content}graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])graph_builder.add_edge(START, \"analyze_query\")graph = graph_builder.compile()API Reference:hub | WebBaseLoader | Document | InMemoryVectorStore | RecursiveCharacterTextSplitter | StateGraph\n",
      "display(Image(graph.get_graph().draw_mermaid_png()))\n",
      "\n",
      "We can test our implementation by specifically asking for context from the end of the post. Note that the model includes different information in its answer.\n",
      "for step in graph.stream(    {\"question\": \"What does the end of the post say about Task Decomposition?\"},    stream_mode=\"updates\",):    print(f\"{step}\\n\\n----------------\\n\")\n",
      "{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'end'}}}----------------{'retrieve': {'context': [Document(id='d6cef137-e1e8-4ddc-91dc-b62bd33c6020', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39221, 'section': 'end'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'), Document(id='d1834ae1-eb6a-43d7-a023-08dfa5028799', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39086, 'section': 'end'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(id='ca7f06e4-2c2e-4788-9a81-2418d82213d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 32942, 'section': 'end'}, page_content='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'), Document(id='1fcc2736-30f4-4ef6-90f2-c64af92118cb', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 35127, 'section': 'end'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease')]}}----------------{'generate': {'answer': 'The end of the post highlights that task decomposition faces challenges in long-term planning and adapting to unexpected errors. LLMs struggle with adjusting their plans, making them less robust compared to humans who learn from trial and error. This indicates a limitation in effectively exploring the solution space and handling complex tasks.'}}----------------\n",
      "In both the streamed steps and the LangSmith trace, we can now observe the structured query that was fed into the retrieval step.\n",
      "Query Analysis is a rich problem with a wide range of approaches. Refer to the how-to guides for more examples.\n",
      "Next steps​\n",
      "We've covered the steps to build a basic Q&A app over data:\n",
      "\n",
      "Loading data with a Document Loader\n",
      "Chunking the indexed data with a Text Splitter to make it more easily usable by a model\n",
      "Embedding the data and storing the data in a vectorstore\n",
      "Retrieving the previously stored chunks in response to incoming questions\n",
      "Generating an answer using the retrieved chunks as context.\n",
      "\n",
      "In Part 2 of the tutorial, we will extend the implementation here to accommodate conversation-style interactions and multi-step retrieval processes.\n",
      "Further reading:\n",
      "\n",
      "Return sources: Learn how to return source documents\n",
      "Streaming: Learn how to stream outputs and intermediate steps\n",
      "Add chat history: Learn how to add chat history to your app\n",
      "Retrieval conceptual guide: A high-level overview of specific retrieval techniques\n",
      "Edit this pagePreviousTaggingNextBuild a semantic search engineOverviewIndexingRetrieval and generationSetupJupyter NotebookInstallationLangSmithComponentsPreviewDetailed walkthrough1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationQuery analysisNext stepsCommunityLangChain ForumTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\n",
      "\n",
      "\n",
      "Human: O que é SearchIntroductionTutorialsBuild?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \"Search Introduction Tutorials Build\" não parece ser uma expressão comum ou um termo específico em inglês. É possível que esteja faltando alguma informação importante para entender corretamente o significado.\\n\\nSe estiver se referindo a uma página de introdução às aulas de aprendizagem sobre como usar um tutorial de busca, poderia ser traduzida como \"Introdução das Aulas de Tutorial de Busca\". Se for um recurso específico em algum sistema de busca ou plataforma de aprendizado, é possível que esteja faltando a informação necessária para identificar o contexto.\\n\\nSe alguém puder fornecer mais informações sobre onde e em qual contexto esta expressão está sendo usada, seria possível fornecer uma tradução mais precisa e apropriada.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perguntaa = \"O que é SearchIntroductionTutorialsBuild?\"\n",
    "\n",
    "chain.run(input_documents = docu, question = perguntaa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agente-pdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
