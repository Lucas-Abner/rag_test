{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e788ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da41393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caminho = \"files/comunicando.pdf\"\n",
    "loader = PyPDFLoader(caminho)\n",
    "paginas = loader.load()\n",
    "len(paginas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f10f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=5,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = recur_split.split_documents(paginas)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08e1f043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9530f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "diretorio = \"files/chroma_vectorstore\"\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding = embeddings_model,\n",
    "    persist_directory=diretorio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d684d126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8216efe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65726/3326131476.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "vector_store = Chroma(\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=diretorio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8054c2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pergunta = \"Como instalar o Ollama?\"\n",
    "\n",
    "docs = vector_store.similarity_search(pergunta, k=5)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8706eb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singularity exec ollama.sif ollama run <modelo_llm> \n",
      "Agora sim, temos o modelo pronto na máquina remota para realizar requisições. \n",
      " \n",
      " \n",
      "4. Como Requisitar na sua Máquina? \n",
      "Da sua máquina, você vai realizar da mesma maneira que foi citado anteriormente: \n",
      "singularity exec ollama.sif ollama run <modelo_llm> \n",
      "Dessa maneira instalamos o modelo no container através de sua máquina, para aí sim você pode\n",
      "======{'author': 'Lucas Abner', 'creationdate': '2025-07-10T11:05:46-07:00', 'creator': 'Microsoft Word', 'moddate': '2025-07-10T11:05:46-07:00', 'page': 1, 'page_label': '2', 'producer': 'PyPDF', 'source': 'files/comunicando.pdf', 'total_pages': 3}\n",
      "Agora que temos o container em nossa máquina remota, e já fizemos o túnel necessario para escutar \n",
      "a requisição ollama, é preciso ativar esse container para conseguir usar a sua LLM no projeto, certo? \n",
      "Então vamos lá: \n",
      "singularity run –-nv ollama.sif & \n",
      "Viu? De maneira simples já colocamos nosso container para funcionar. Esse // & // comercial é para \n",
      "que você possa usar o mesmo terminal para fazer outras requisições. Mas ainda falta baixar o \n",
      "modelo LLM para requisitar, então vamos fazer:\n",
      "======{'author': 'Lucas Abner', 'creationdate': '2025-07-10T11:05:46-07:00', 'creator': 'Microsoft Word', 'moddate': '2025-07-10T11:05:46-07:00', 'page': 1, 'page_label': '2', 'producer': 'PyPDF', 'source': 'files/comunicando.pdf', 'total_pages': 3}\n",
      "Dessa maneira você consegue enviar seu container para as máquinas. \n",
      " \n",
      " \n",
      "2. Túnel Entre Portas Ollama no Marvin \n",
      "No passo anterior, nós enviamos o container já buildado com o ollama para o servidor HPC(Marvin). \n",
      "Agora como próximo passo, vamos criar um túnel de conexão entre a porta ollama da sua máquina \n",
      "(:11434) com a porta ollama do container que está no Marvin: \n",
      "ssh –L 11434:localhost:11434 <seu_usuario>@<servidor_hpc> \n",
      "Aqui, estamos:\n",
      "======{'author': 'Lucas Abner', 'creationdate': '2025-07-10T11:05:46-07:00', 'creator': 'Microsoft Word', 'moddate': '2025-07-10T11:05:46-07:00', 'page': 0, 'page_label': '1', 'producer': 'PyPDF', 'source': 'files/comunicando.pdf', 'total_pages': 3}\n",
      "CONSUMINDO LLMs DENTRO DO MARVIN \n",
      " \n",
      "Vamos descrever nessa documentação como enviar arquivos/containers para outra máquina e a \n",
      "fazer túnel entre portas. \n",
      " \n",
      "1.  Enviando Container para o Marvin \n",
      "Anteriormente, criamos um container singularity, onde está configurado para uso de GPUs \n",
      "dedicados ao Ollama. Então vamos passar esse arquivo para nosso HPC para que ele rode o Ollama \n",
      "e requisite direto do Servidor. Para mandar arquivos de sua máquina para outra é bem simples:\n",
      "======{'author': 'Lucas Abner', 'creationdate': '2025-07-10T11:05:46-07:00', 'creator': 'Microsoft Word', 'moddate': '2025-07-10T11:05:46-07:00', 'page': 0, 'page_label': '1', 'producer': 'PyPDF', 'source': 'files/comunicando.pdf', 'total_pages': 3}\n",
      "scp ollama.sif <seu_usuario>@<servidor_hpc>:<caminho_para_deixar_arquivo> \n",
      "O que estamos fazendo aqui? Estamos transferindo uma cópia da sua máquina para o hpc, onde: \n",
      "• scp = é a cópia que você está fazendo \n",
      "• Ollama.sif = nome do arquivo que você quer transferir (definir o caminho do arquivo) \n",
      "• <seu_usuario> = seu usuario no HPC \n",
      "• <servidor_hpc> = nome do HPC \n",
      "• <caminho_para_deixar_arquivo> = onde você quer deixar o arquivo no HPC\n",
      "======{'author': 'Lucas Abner', 'creationdate': '2025-07-10T11:05:46-07:00', 'creator': 'Microsoft Word', 'moddate': '2025-07-10T11:05:46-07:00', 'page': 0, 'page_label': '1', 'producer': 'PyPDF', 'source': 'files/comunicando.pdf', 'total_pages': 3}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(f\"======{doc.metadata}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agente-pdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
